{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search \"end of functions\" and then you can run all cells up until that to get the functions loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lLMUX4tWGIh6"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import functools\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "# from google.colab import files\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "m1IShAoGxsRk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IZSLbKXbx1ML"
   },
   "outputs": [],
   "source": [
    "import edward2 as ed\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "dtype = tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4re71q0_QgW9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickl\\anaconda3\\lib\\site-packages\\gpflow\\experimental\\utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "C:\\Users\\nickl\\anaconda3\\lib\\site-packages\\gpflow\\experimental\\utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import gpflow as gpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jSdFWIlbH8lC"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress pfor warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSk3pmZIfetu",
    "outputId": "3b00f277-cac0-4365-9396-c30505cdf89e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0. Expected: 2.7.0\n",
      "TensorFlow Probability version: 0.18.0. Expected: 0.15.0\n"
     ]
    }
   ],
   "source": [
    "# Verify versions.\n",
    "print(f'TensorFlow version: {tf.__version__}. Expected: 2.7.0')\n",
    "print(f'TensorFlow Probability version: {tfp.__version__}. Expected: 0.15.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEjMYnKjCUdj"
   },
   "source": [
    "Enable GPU support by selecting `Edit > Notebook Settings > Hardware accelerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "y8ERsv-xCUtO",
    "outputId": "d8350d89-1b08-465d-8971-d100557d1bec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this unfortunately doesn't carry over functions loaded in well, so I'm skipping it\n",
    "#import bne_0_b_wrapper_functions\n",
    "#import bne_0_c_core_model_functions\n",
    "#import bne_0_d_utility_functions\n",
    "#import bne_0_e_simulation_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRgKTbARZuN1"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3ea0MaYzlhn"
   },
   "source": [
    "## High-level Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Z7tmXy9tI-E0"
   },
   "outputs": [],
   "source": [
    "# @title Wrapper: generate_data_1d\n",
    "OutcomeDistribution = Callable[[np.ndarray, np.ndarray], Any]\n",
    "\n",
    "def generate_data_1d(N_train: int, \n",
    "                     N_base: int = 50, \n",
    "                     N_test: int = 1000,\n",
    "                     y_dist: OutcomeDistribution = np.random.lognormal, \n",
    "                     seed: int = 0):\n",
    "  \"\"\"Generates 1D data for training ensemble model.\n",
    "  \n",
    "  Args:\n",
    "    N_train: Number of extra training data points for ensemble model.\n",
    "    N_base: Number of data points to train base model.\n",
    "    N_test: Number of testing locations.\n",
    "    y_dist: The data generating distribution of y that takes into two \n",
    "      vector-valued arguments (e.g., mean and variance).\n",
    "    seed: Random seed\n",
    "  \n",
    "  Returns: \n",
    "    The (X, Y) data for base model, ensemble training, and ensemble testing.\n",
    "  \"\"\"\n",
    "  np.random.seed(seed)\n",
    "  tf.random.set_seed(seed)\n",
    "  \n",
    "  # Data ranges.\n",
    "  train_range = (-3 * np.pi, 3 * np.pi)\n",
    "  test_range = (-5 * np.pi, 5 * np.pi)\n",
    "\n",
    "  # Build inputs X\n",
    "  process_X_fn = lambda X: np.sort(X).astype(np.float32)\n",
    "\n",
    "  X_base = process_X_fn(\n",
    "      np.random.uniform(*train_range, size=(N_base, 1)))  # Data for training base model.\n",
    "  X_train_extra = process_X_fn(\n",
    "      np.random.uniform(*train_range, size=(N_train, 1)))  # Additional training data for ensemble.\n",
    "  X_train = np.concatenate([X_base, X_train_extra])\n",
    "  X_test = process_X_fn(\n",
    "      np.random.uniform(*test_range, size=(N_test, 1)))  # Testing data covering whole feature space.\n",
    "  X_test = np.sort(X_test, axis=0)  # Sort testing data for visualization.\n",
    "\n",
    "  # True functions for moments.\n",
    "  f_mean = lambda x: np.sin(x)\n",
    "  f_stddev = lambda x: tf.exp(np.cos(x) - 1.0).numpy()\n",
    "\n",
    "  # Compute loc and scale as functions of input X\n",
    "  loc_base = f_mean(X_base)\n",
    "  loc_train = f_mean(X_train)\n",
    "  loc_test = f_mean(X_test)\n",
    "\n",
    "  std_base = f_stddev(X_base)\n",
    "  std_train = f_stddev(X_train)\n",
    "  std_test = f_stddev(X_test)\n",
    "\n",
    "  # Sample outputs Y and convert to float32 for TF compatibility.\n",
    "  Y_base = y_dist(loc_base, std_base).astype(np.float32)\n",
    "  Y_train = y_dist(loc_train, std_train).astype(np.float32)\n",
    "  Y_test = y_dist(loc_test, std_test).astype(np.float32)\n",
    "\n",
    "  # Compute true mean and variance.\n",
    "  N_test_sample = 1000\n",
    "  Y_test_sample = np.stack(\n",
    "      [y_dist(loc_test, std_test) for _ in range(N_test_sample)])\n",
    "  mean_test = np.mean(Y_test_sample, axis=0)\n",
    "\n",
    "  return X_base, X_train, X_test, Y_base, Y_train, Y_test, mean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9JJvGSPJQ-3",
    "outputId": "42ca75b8-ade8-476f-df87-791f828e5144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n",
      "(150, 1)\n",
      "(1000, 1)\n",
      "(50, 1)\n",
      "(150, 1)\n",
      "(1000, 1)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# @title Examine \"generate_data_1d\"\n",
    "gen_1d_result = generate_data_1d(N_train=100, N_base=50, N_test= 1000, y_dist= np.random.lognormal, seed = 0)\n",
    "\"\"\"Generates 1D data for training ensemble model.\n",
    "  \n",
    "  Args:\n",
    "    N_train: Number of extra training data points for ensemble model.\n",
    "    N_base: Number of data points to train base model.\n",
    "    N_test: Number of testing locations.\n",
    "    y_dist: The data generating distribution of y that takes into two \n",
    "      vector-valued arguments (e.g., mean and variance).\n",
    "    seed: Random seed\n",
    "  \n",
    "  Returns: \n",
    "    The (X, Y) data for base model, ensemble training, and ensemble testing.\n",
    "  \"\"\"\n",
    "for dataset in gen_1d_result:\n",
    "  print(dataset.shape)\n",
    "  #print(dataset.ravel())\n",
    "  # X_base, X_train, X_test, Y_base, Y_train, Y_test, mean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "llTow_oNJ1I-"
   },
   "outputs": [],
   "source": [
    "# @title Wrapper: generate_data_2d\n",
    "def generate_data_2d(N_train: int, \n",
    "                     N_base: int = 100, \n",
    "                     N_test: int = 5000,\n",
    "                     y_dist: OutcomeDistribution = np.random.lognormal, \n",
    "                     seed: int = 0,\n",
    "                     explicit_skewness: bool = False):\n",
    "  \"\"\"Generates 2D data for training ensemble model.\n",
    "  \n",
    "  Args:\n",
    "    N_train: Number of extra training data points for ensemble model.\n",
    "    N_base: Number of data points to train base model.\n",
    "    N_test: Number of testing locations.\n",
    "    y_dist: The data generating distribution of y that takes into two \n",
    "      vector-valued arguments (e.g., mean and variance).\n",
    "    seed: Random seed.\n",
    "    explicit_skewness: Whether to explicitly introduce skewness.\n",
    "  \n",
    "  Returns: \n",
    "    The (X, Y) data for base model, ensemble training, and ensemble testing.\n",
    "  \"\"\"\n",
    "  np.random.seed(seed)\n",
    "  tf.random.set_seed(seed)\n",
    "\n",
    "  # Data ranges.\n",
    "  train_range = (-np.pi, np.pi)\n",
    "  test_range = (-1.25 * np.pi, 1.25 * np.pi)\n",
    "\n",
    "  # Build inputs X\n",
    "  sample_X = lambda low, high, n: np.random.uniform(low, high, size=(n, 2)).astype(np.float32)\n",
    "\n",
    "  X_base = sample_X(*train_range, N_base)  # Data for training base model.\n",
    "  X_train_extra = sample_X(*train_range, N_train)  # Additional training data for ensemble.\n",
    "  X_train = np.concatenate([X_base, X_train_extra])\n",
    "  X_test = sample_X(*test_range, N_test)  # Testing data covering whole feature space.\n",
    "\n",
    "  f_mean = lambda x, mplr=1.5: bird(x[:,0:1]*mplr, x[:,1:2]*mplr)/5.  # mplr 1.25 to 1.5\n",
    "  # f_std = lambda x, mplr=1.0: townsend(x[:,0:1]*mplr, x[:,1:2]*mplr)/5.\n",
    "  f_std = lambda x, mplr=0.5: rosenbrock(x[:,0:1]*mplr, x[:,1:2]*mplr)  # mplr 0.25 to 0.5\n",
    "  f_skew = lambda x, mplr=0.5: np.clip(\n",
    "      townsend(x[:,0:1]*mplr, x[:,1:2]*mplr), a_min=1e-5, a_max=np.inf)\n",
    "\n",
    "  # Generate mean, std and skewness surfaces.\n",
    "  loc_base, loc_train, loc_test = map(f_mean, [X_base, X_train, X_test])\n",
    "  std_base, std_train, std_test = map(f_std, [X_base, X_train, X_test])\n",
    "  skew_base, skew_train, skew_test = map(f_skew, [X_base, X_train, X_test])\n",
    "\n",
    "  # DEBUG: make std very small.\n",
    "  std_base = 1e-3\n",
    "  std_train = 1e-3\n",
    "  std_test = 1e-3\n",
    "\n",
    "  dist_args_base = (loc_base, std_base)\n",
    "  dist_args_train = (loc_train, std_train)\n",
    "  dist_args_test = (loc_test, std_test)\n",
    "\n",
    "  if explicit_skewness:\n",
    "    dist_args_base += (skew_base,)\n",
    "    dist_args_train += (skew_train,)\n",
    "    dist_args_test += (skew_test,)\n",
    "\n",
    "  # Sample outputs Y and convert to float32 for TF compatibility.\n",
    "  Y_base = y_dist(*dist_args_base).astype(np.float32)\n",
    "  Y_train = y_dist(*dist_args_train).astype(np.float32)\n",
    "  Y_test = y_dist(*dist_args_test).astype(np.float32)\n",
    "\n",
    "  # Compute true mean and variance.\n",
    "  N_test_sample = 1000\n",
    "  Y_test_sample = np.stack(\n",
    "      [y_dist(*dist_args_test) for _ in range(N_test_sample)])\n",
    "  mean_test = np.mean(Y_test_sample, axis=0)\n",
    "  # std_test = np.std(Y_test_sample, axis=0)\n",
    "\n",
    "  return X_base, X_train, X_test, Y_base, Y_train, Y_test, mean_test\n",
    "\n",
    "# Helper functions.\n",
    "bird = lambda x, y: (  # Mishra's Bird function.\n",
    "    np.sin(y) * np.exp((1.-np.cos(x))**2) + \n",
    "    np.cos(x) * np.exp((1.-np.sin(y))**2) + (x-y)**2)/100. - 1.\n",
    "townsend = lambda x, y: ((  # (Modified) Townsend function.\n",
    "    -(np.cos(x - 1.) * y)**2 - x * np.sin(3 * x + y)) + 10.)\n",
    "rosenbrock = lambda x, y: (  # (Modified) Rosenbrock function.\n",
    "    100. * (y - x**2)**2 + (1 - x)**2)/25.\n",
    "goldstein = lambda x, y: (\n",
    "    1. + (x + y + 1.)**2 * (19. - 14.*x + 3.*x**2 - 14.*y + 6*x*y + 3. * y**2)\n",
    "    ) * (30. + (2.*x - 3.*y)**2 * (18. - 32.*x + 12.*x**2 + 48.*y - 36.*x*y + 27.*y**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWcGM7gZNJJw",
    "outputId": "91810dd3-aeee-4f79-99e0-ff00f5a30a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(600, 2)\n",
      "(5000, 2)\n",
      "(100, 1)\n",
      "(600, 1)\n",
      "(5000, 1)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "# @title Examine \"generate_data_2d\"\n",
    "gen_2d_result = generate_data_2d(N_train= 500,  N_base= 100, N_test= 5000, y_dist= np.random.lognormal, \n",
    "                     seed= 0, explicit_skewness = False)\n",
    "\"\"\"Generates 2D data for training ensemble model.\n",
    "  \n",
    "  Args:\n",
    "    N_train: Number of extra training data points for ensemble model.\n",
    "    N_base: Number of data points to train base model.\n",
    "    N_test: Number of testing locations.\n",
    "    y_dist: The data generating distribution of y that takes into two \n",
    "      vector-valued arguments (e.g., mean and variance).\n",
    "    seed: Random seed.\n",
    "    explicit_skewness: Whether to explicitly introduce skewness.\n",
    "  \n",
    "  Returns: \n",
    "    The (X, Y) data for base model, ensemble training, and ensemble testing.\n",
    "  \"\"\"\n",
    "for dataset in gen_2d_result:\n",
    "  print(dataset.shape)\n",
    "  #print(dataset)\n",
    "  # X_base, X_train, X_test, Y_base, Y_train, Y_test, mean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "uxvcNlilzoPz"
   },
   "outputs": [],
   "source": [
    "# @title Wrapper: run_posterior_inference\n",
    "def run_posterior_inference(model_dist: tfd.Distribution, \n",
    "                            Y: tf.Tensor, \n",
    "                            mcmc_config: Dict[str, Any], \n",
    "                            map_config: Optional[Dict[str, Any]] = None, \n",
    "                            model_config: Optional[Dict[str, Any]] = None,\n",
    "                            initialize_from_map: bool = True):\n",
    "  \"\"\"Wrapper function for running MCMC with MAP initialization.\"\"\"\n",
    "  # Defines posterior log likelihood function, and also a \n",
    "  # randomly-sampled initial state from model prior.\n",
    "  nchain = mcmc_config['nchain']\n",
    "  init_state, target_log_prob_fn = prepare_mcmc(model_dist, Y, nchain=nchain)  \n",
    "  \n",
    "  if initialize_from_map:\n",
    "    # Initializes at MAP, shape (num_chains, param_shape_0, param_shape_1).\n",
    "    print('Running MAP:', end='\\t')\n",
    "    init_state = run_map(target_log_prob_fn=target_log_prob_fn, \n",
    "                         gp_config=model_config,\n",
    "                         **map_config)\n",
    "\n",
    "    init_state = tf.stack([init_state] * mcmc_nchain, axis=0)\n",
    "\n",
    "  # Run MCMC, shape (param_shape_0, param_shape_1, num_chains).\n",
    "  print('Running MCMC:', end='\\t')\n",
    "  gp_w_samples, _ = run_mcmc(init_state=init_state,\n",
    "                             target_log_prob_fn=target_log_prob_fn,\n",
    "                             **mcmc_config)  \n",
    "  \n",
    "  return gp_w_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "1oe8ndFdVYAD"
   },
   "outputs": [],
   "source": [
    "# @title Wrapper: run_base_models\n",
    "def run_base_models(X_base, X_train, X_test, \n",
    "                    Y_base, Y_train, Y_test, kernels=None, \n",
    "                    num_train_steps=100, debug_mode=False):\n",
    "  if kernels is None:\n",
    "    kernels = [gpf.kernels.Matern52(lengthscales=0.5), \n",
    "               gpf.kernels.Polynomial(degree=3.), \n",
    "               gpf.kernels.ArcCosine(weight_variances=1., bias_variance=1.),\n",
    "               gpf.kernels.Periodic(gpf.kernels.Exponential(lengthscales=2.))]\n",
    "\n",
    "  n_models = len(kernels)\n",
    "  kernel_names = [kernel.name for kernel in kernels]  \n",
    "\n",
    "  models = [\n",
    "    get_base_prediction(X_base, Y_base, X_train, kernel=k,\n",
    "                        num_train_steps=num_train_steps, \n",
    "                        debug_mode=debug_mode) for k in kernels]\n",
    "  base_preds_train = tf.stack([\n",
    "    get_base_prediction(X_base, Y_base, X_train, model=m, debug_mode=debug_mode) for m in models], axis=-1)\n",
    "  base_preds_test = tf.stack([\n",
    "    get_base_prediction(X_base, Y_base, X_test, model=m, debug_mode=debug_mode) for m in models], axis=-1)\n",
    "  \n",
    "  return base_preds_train, base_preds_test, kernel_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "dklNu6l3lTdZ"
   },
   "outputs": [],
   "source": [
    "# @title Wrapper: run_bma_model\n",
    "def run_bma_model(X_train, X_test, Y_train,\n",
    "                  base_preds_train, base_preds_test,\n",
    "                  gp_lengthscale=1.,\n",
    "                  gp_l2_regularizer=0.1,\n",
    "                  y_noise_std=0.1,\n",
    "                  map_step_size=0.1,\n",
    "                  map_num_steps=10_000,\n",
    "                  mcmc_step_size=0.1,\n",
    "                  mcmc_num_steps=10_000,\n",
    "                  mcmc_nchain=10,\n",
    "                  mcmc_burnin=2_500,\n",
    "                  mcmc_initialize_from_map=False,\n",
    "                  n_samples_eval=1000,\n",
    "                  n_samples_train=100,\n",
    "                  n_samples_test=200,\n",
    "                  return_mcmc_examples=True,\n",
    "                  seed=0, \n",
    "                  debug_mode=False):\n",
    "  # Assemble model configs.\n",
    "  model_config = DEFAULT_GP_CONFIG.copy()\n",
    "  map_config = DEFAULT_MAP_CONFIG.copy()\n",
    "  mcmc_config = DEFAULT_MCMC_CONFIG.copy()\n",
    "\n",
    "  model_config['lengthscale']=gp_lengthscale  \n",
    "  model_config['l2_regularizer']=gp_l2_regularizer\n",
    "  model_config['y_noise_std']=y_noise_std\n",
    "\n",
    "  map_config['learning_rate']=map_step_size\n",
    "  map_config['num_steps']=map_num_steps\n",
    "\n",
    "  mcmc_config['nchain']=mcmc_nchain\n",
    "  mcmc_config['burnin']=mcmc_burnin\n",
    "  mcmc_config['step_size']=mcmc_step_size\n",
    "  mcmc_config['num_steps']=mcmc_num_steps\n",
    "  \n",
    "  # Model prior.\n",
    "  bma_prior, gp_config = bma_dist(X_train, \n",
    "                                  base_preds_train, \n",
    "                                  **model_config)\n",
    "\n",
    "  model_config.update(gp_config)  \n",
    "  if debug_mode:\n",
    "    print(model_config)\n",
    "\n",
    "  # Run MCMC estimation.\n",
    "  weight_samples = run_posterior_inference(\n",
    "      model_dist=bma_prior, \n",
    "      model_config=model_config,\n",
    "      Y=Y_train, \n",
    "      map_config=map_config,\n",
    "      mcmc_config=mcmc_config,\n",
    "      initialize_from_map=mcmc_initialize_from_map)  \n",
    "  \n",
    "  del bma_prior\n",
    "  # Get posterior sample for all model parameters.\n",
    "  bma_joint_samples = make_bma_samples(X_test, None, \n",
    "                                       base_preds_test, \n",
    "                                       bma_weight_samples=weight_samples[0],\n",
    "                                       bma_model_config=model_config, \n",
    "                                       n_samples=n_samples_eval, \n",
    "                                       seed=seed,\n",
    "                                       y_samples_only=False,\n",
    "                                       debug_mode=debug_mode)\n",
    "\n",
    "  if return_mcmc_examples:\n",
    "    # MCMC data for BNE training, shapes (num_samples * num_data, ...)\n",
    "    means_train_mcmc, X_train_mcmc, Y_train_mcmc = make_bma_samples(\n",
    "        X_train, Y_train, \n",
    "        base_preds_train, \n",
    "        bma_weight_samples=weight_samples[0],\n",
    "        bma_model_config=model_config,\n",
    "        n_samples=n_samples_train,\n",
    "        seed=seed, \n",
    "        prepare_mcmc_training=True,\n",
    "        debug_mode=debug_mode)\n",
    "\n",
    "    # MCMC data for BNE testing, shape (num_samples, num_data, num_output).\n",
    "    means_test_mcmc = make_bma_samples(X_test, None, \n",
    "                                       base_preds_test, \n",
    "                                       bma_weight_samples=weight_samples[0],\n",
    "                                       bma_model_config=model_config,\n",
    "                                       n_samples=n_samples_test,\n",
    "                                       seed=seed,\n",
    "                                       debug_mode=debug_mode)\n",
    "    \n",
    "    return (bma_joint_samples, X_train_mcmc, Y_train_mcmc, \n",
    "            means_train_mcmc, means_test_mcmc)\n",
    "    \n",
    "  return bma_joint_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "5ODepIWHJ1fs"
   },
   "outputs": [],
   "source": [
    "# @title Wrapper: run_bne_model\n",
    "BNE_MOMENT_TYPES = ('none', 'mean', 'variance', 'skewness')\n",
    "def run_bne_model(X_train: tf.Tensor,\n",
    "                  Y_train: tf.Tensor,\n",
    "                  X_test: tf.Tensor, \n",
    "                  base_model_samples_train: tf.Tensor,\n",
    "                  base_model_samples_test: tf.Tensor,\n",
    "                  moment_mode: str = 'none',\n",
    "                  gp_lengthscale: float = 1., \n",
    "                  gp_l2_regularizer: float = 10., \n",
    "                  variance_prior_mean: float = 0., \n",
    "                  skewness_prior_mean: float = 0.,\n",
    "                  map_step_size: float = 5e-3,\n",
    "                  map_num_steps: float = 10_000,\n",
    "                  mcmc_step_size: float = 1e-2,\n",
    "                  mcmc_num_steps: float = 10_000,\n",
    "                  mcmc_burnin: int = 2_500,\n",
    "                  mcmc_nchain: int = 10,\n",
    "                  mcmc_initialize_from_map: bool = False,\n",
    "                  seed: int = 0,\n",
    "                  debug_mode: bool = False):\n",
    "  \"\"\"Runs the full BNE model end-to-end.\n",
    "\n",
    "  This function performs end-to-end estimation of BNE model. It supports four \n",
    "  modes as determined by `moment_mode`:\n",
    "\n",
    "    * 'none': Estimate a constant-variance model y ~ N(m(x), scale).\n",
    "    * 'mean': Estimate a residual process model y ~ N(m(x) + r(x), scale).\n",
    "    * 'variance': Estimate a heterogeneous residual process model \n",
    "        y ~ N(m(x) + r(x), scale(x)).\n",
    "    * 'skewness': Estimate a heterogeneous residual process model with \n",
    "        skewness y ~ EMG(m(x) + r(x), scale(x), rate(x)).\n",
    "\n",
    "  Args:\n",
    "    X_train: The training features (num_train, num_dim).\n",
    "    Y_train: The training responses (num_train, 1).\n",
    "    X_test: The testing features (num_test, num_dim).\n",
    "    base_model_samples_train: The posterior samples m(x) from the base ensemble \n",
    "      model at train locations, shape (num_train, 1).\n",
    "    base_model_samples_test: The posterior samples m(x) from the base ensemble \n",
    "      model at test locations, shape (num_test, 1).\n",
    "    moment_mode: The highest moment for the BNE model to estimate. \n",
    "      Must be one of ('none', 'mean', 'variance', 'skewness').\n",
    "    gp_lengthscale:  length scale for model Gaussian processes.\n",
    "    gp_l2_regularizer: l2_regularizer for model Gaussian processes.\n",
    "    bne_variance_prior_mean: Prior mean to variance parameter.  \n",
    "    bne_skewness_prior_mean: Prior mean to skewness parameter.\n",
    "    map_step_size: Step size for MAP optimization.\n",
    "    map_num_steps: Number of train steps for MAP optimization. \n",
    "    mcmc_step_size: Step size for HMC sampling\n",
    "    mcmc_num_steps: Number of train steps for HMC sampling.\n",
    "    seed: The seed for generating posterior samples.\n",
    "\n",
    "  Returns:\n",
    "    A dictionary with model parameters and its MCMC samples. \n",
    "  \"\"\"\n",
    "  # Prepares model and estimation configs.\n",
    "  gp_config = DEFAULT_GP_CONFIG.copy()\n",
    "  model_config = DEFAULT_BNE_CONFIG.copy()\n",
    "  map_config = DEFAULT_MAP_CONFIG.copy()\n",
    "  mcmc_config = DEFAULT_MCMC_CONFIG.copy()\n",
    "\n",
    "  gp_config['lengthscale'] = gp_lengthscale\n",
    "  gp_config['l2_regularizer'] = gp_l2_regularizer\n",
    "\n",
    "  model_config['estimate_mean'] = moment_mode in ('mean', 'variance', 'skewness')\n",
    "  model_config['estimate_variance'] = moment_mode in ('variance', 'skewness')\n",
    "  model_config['estimate_skewness'] = moment_mode in ('skewness',)\n",
    "  model_config['variance_prior_mean'] = variance_prior_mean\n",
    "  model_config['skewness_prior_mean'] = skewness_prior_mean\n",
    "  model_config.update(gp_config)\n",
    "\n",
    "  map_config['learning_rate'] = map_step_size\n",
    "  map_config['num_steps']=map_num_steps\n",
    "\n",
    "  mcmc_config['nchain'] = mcmc_nchain\n",
    "  mcmc_config['burnin'] = mcmc_burnin\n",
    "  mcmc_config['step_size'] = mcmc_step_size\n",
    "  mcmc_config['num_steps'] = mcmc_num_steps\n",
    "\n",
    "  # Constructs prior distribution.\n",
    "  bne_prior, bne_gp_config = bne_model_dist(X_train,\n",
    "                                            mean_preds=base_model_samples_train,\n",
    "                                            **model_config)\n",
    "\n",
    "  model_config.update(bne_gp_config)\n",
    "\n",
    "  # Estimates GP weight posterior using MCMC.\n",
    "  gp_weight_samples = run_posterior_inference(\n",
    "      model_dist=bne_prior,\n",
    "      model_config=bne_gp_config,\n",
    "      Y=Y_train,\n",
    "      map_config=map_config,\n",
    "      mcmc_config=mcmc_config,\n",
    "      initialize_from_map=mcmc_initialize_from_map)\n",
    "  \n",
    "  # Generates the e for all model parameters. \n",
    "  joint_samples = make_bne_samples(X_test,\n",
    "                                   mean_preds=base_model_samples_test,\n",
    "                                   bne_model_config=model_config,\n",
    "                                   bne_weight_samples=gp_weight_samples[0],\n",
    "                                   seed=seed,\n",
    "                                   debug_mode=debug_mode)\n",
    "  del bne_prior  \n",
    "  return joint_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLDm7Mj5yWLD"
   },
   "source": [
    "## Core Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "ZSep37BMfz64"
   },
   "outputs": [],
   "source": [
    "# @title Distribution: ExponentiallyModifiedGaussian\n",
    "from tensorflow_probability.python.bijectors import identity as identity_bijector\n",
    "from tensorflow_probability.python.bijectors import softplus as softplus_bijector\n",
    "from tensorflow_probability.python.distributions import distribution\n",
    "from tensorflow_probability.python.distributions import exponential as exponential_lib\n",
    "from tensorflow_probability.python.distributions import normal as normal_lib\n",
    "from tensorflow_probability.python.internal import assert_util\n",
    "from tensorflow_probability.python.internal import dtype_util\n",
    "from tensorflow_probability.python.internal import parameter_properties\n",
    "from tensorflow_probability.python.internal import reparameterization\n",
    "from tensorflow_probability.python.internal import samplers\n",
    "from tensorflow_probability.python.internal import special_math\n",
    "from tensorflow_probability.python.internal import tensor_util\n",
    "from tensorflow_probability.python.math import generic as tfp_math\n",
    "\n",
    "\n",
    "class ExponentiallyModifiedGaussian(\n",
    "    distribution.AutoCompositeTensorDistribution):\n",
    "  \"\"\"Exponentially modified Gaussian distribution.\n",
    "  #### Mathematical details\n",
    "  The exponentially modified Gaussian distribution is the sum of a normal\n",
    "  distribution and an exponential distribution.\n",
    "  ```none\n",
    "  X ~ Normal(loc, scale)\n",
    "  Y ~ Exponential(rate)\n",
    "  Z = X + Y\n",
    "  ```\n",
    "  is equivalent to\n",
    "  ```none\n",
    "  Z ~ ExponentiallyModifiedGaussian(loc, scale, rate)\n",
    "  ```\n",
    "  #### Examples\n",
    "  ```python\n",
    "  tfd = tfp.distributions\n",
    "  # Define a single scalar ExponentiallyModifiedGaussian distribution\n",
    "  dist = tfd.ExponentiallyModifiedGaussian(loc=0., scale=1., rate=3.)\n",
    "  # Evaluate the pdf at 1, returing a scalar.\n",
    "  dist.prob(1.)\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               loc,\n",
    "               scale,\n",
    "               rate,\n",
    "               validate_args=False,\n",
    "               allow_nan_stats=True,\n",
    "               name='ExponentiallyModifiedGaussian'):\n",
    "    \"\"\"Construct an exponentially-modified Gaussian distribution.\n",
    "    The Gaussian distribution has mean `loc` and stddev `scale`,\n",
    "    and Exponential distribution has rate parameter `rate`.\n",
    "    The parameters `loc`, `scale`, and `rate` must be shaped in a way that\n",
    "    supports broadcasting (e.g. `loc + scale + rate` is a valid operation).\n",
    "    Args:\n",
    "      loc: Floating-point `Tensor`; the means of the distribution(s).\n",
    "      scale: Floating-point `Tensor`; the stddevs of the distribution(s). Must\n",
    "        contain only positive values.\n",
    "      rate: Floating-point `Tensor`; the rate parameter for the exponential\n",
    "        distribution.\n",
    "      validate_args: Python `bool`, default `False`. When `True` distribution\n",
    "        parameters are checked for validity despite possibly degrading runtime\n",
    "        performance. When `False` invalid inputs may silently render incorrect\n",
    "        outputs.\n",
    "      allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n",
    "        (e.g., mean, mode, variance) use the value \"`NaN`\" to indicate the\n",
    "        result is undefined. When `False`, an exception is raised if one or more\n",
    "        of the statistic's batch members are undefined.\n",
    "      name: Python `str` name prefixed to Ops created by this class.\n",
    "    Raises:\n",
    "      TypeError: if `loc`, `scale`, and `rate` are not all the same `dtype`.\n",
    "    \"\"\"\n",
    "    parameters = dict(locals())\n",
    "    with tf.name_scope(name) as name:\n",
    "      dtype = dtype_util.common_dtype([loc, scale, rate], dtype_hint=tf.float32)\n",
    "      self._loc = tensor_util.convert_nonref_to_tensor(\n",
    "          loc, dtype=dtype, name='loc')\n",
    "      self._scale = tensor_util.convert_nonref_to_tensor(\n",
    "          scale, dtype=dtype, name='scale')\n",
    "      self._rate = tensor_util.convert_nonref_to_tensor(\n",
    "          rate, dtype=dtype, name='rate')\n",
    "      super(ExponentiallyModifiedGaussian, self).__init__(\n",
    "          dtype=dtype,\n",
    "          reparameterization_type=reparameterization.FULLY_REPARAMETERIZED,\n",
    "          validate_args=validate_args,\n",
    "          allow_nan_stats=allow_nan_stats,\n",
    "          parameters=parameters,\n",
    "          name=name)\n",
    "\n",
    "  @staticmethod\n",
    "  def _param_shapes(sample_shape):\n",
    "    return dict(\n",
    "        zip(('loc', 'scale', 'rate'),\n",
    "            ([tf.convert_to_tensor(sample_shape, dtype=tf.int32)] * 3)))\n",
    "\n",
    "  @classmethod\n",
    "  def _parameter_properties(cls, dtype, num_classes=None):\n",
    "    return dict(\n",
    "        loc=parameter_properties.ParameterProperties(),\n",
    "        scale=parameter_properties.ParameterProperties(\n",
    "            default_constraining_bijector_fn=(\n",
    "                lambda: softplus_bijector.Softplus(low=dtype_util.eps(dtype)))),\n",
    "        rate=parameter_properties.ParameterProperties(\n",
    "            default_constraining_bijector_fn=(\n",
    "                lambda: softplus_bijector.Softplus(low=dtype_util.eps(dtype)))))\n",
    "\n",
    "  @property\n",
    "  def loc(self):\n",
    "    \"\"\"Distribution parameter for the mean of the normal distribution.\"\"\"\n",
    "    return self._loc\n",
    "\n",
    "  @property\n",
    "  def scale(self):\n",
    "    \"\"\"Distribution parameter for standard deviation of the normal distribution.\"\"\"\n",
    "    return self._scale\n",
    "\n",
    "  @property\n",
    "  def rate(self):\n",
    "    \"\"\"Distribution parameter for rate parameter of exponential distribution.\"\"\"\n",
    "    return self._rate\n",
    "\n",
    "  def _event_shape_tensor(self):\n",
    "    return tf.constant([], dtype=tf.int32)\n",
    "\n",
    "  def _event_shape(self):\n",
    "    return tf.TensorShape([])\n",
    "\n",
    "  def _sample_n(self, n, seed=None):\n",
    "    normal_seed, exp_seed = samplers.split_seed(seed, salt='emg_sample')\n",
    "    # need to make sure component distributions are broadcast appropriately\n",
    "    # for correct generation of samples\n",
    "    loc = tf.convert_to_tensor(self.loc)\n",
    "    rate = tf.convert_to_tensor(self.rate)\n",
    "    scale = tf.convert_to_tensor(self.scale)\n",
    "    batch_shape = self._batch_shape_tensor(loc=loc, scale=scale, rate=rate)\n",
    "    loc_broadcast = tf.broadcast_to(loc, batch_shape)\n",
    "    rate_broadcast = tf.broadcast_to(rate, batch_shape)\n",
    "    normal_dist = normal_lib.Normal(loc=loc_broadcast, scale=scale)\n",
    "    exp_dist = exponential_lib.Exponential(rate_broadcast)\n",
    "    x = normal_dist.sample(n, normal_seed)\n",
    "    y = exp_dist.sample(n, exp_seed)\n",
    "    return x + y\n",
    "\n",
    "  def _log_prob(self, x):\n",
    "    loc = tf.convert_to_tensor(self.loc)\n",
    "    rate = tf.convert_to_tensor(self.rate)\n",
    "    scale = tf.convert_to_tensor(self.scale)\n",
    "    two = dtype_util.as_numpy_dtype(x.dtype)(2.)\n",
    "    z = (x - loc) / scale\n",
    "    w = rate * scale\n",
    "    return (tf.math.log(rate) + w / two * (w - 2 * z) +\n",
    "            special_math.log_ndtr(z - w))\n",
    "\n",
    "  def _log_cdf(self, x):\n",
    "    rate = tf.convert_to_tensor(self.rate)\n",
    "    scale = tf.convert_to_tensor(self.scale)\n",
    "    x_centralized = x - self.loc\n",
    "    u = rate * x_centralized\n",
    "    v = rate * scale\n",
    "    vsquared = tf.square(v)\n",
    "    return tfp_math.log_sub_exp(\n",
    "        special_math.log_ndtr(x_centralized / scale),\n",
    "        -u + vsquared / 2. + special_math.log_ndtr((u - vsquared) / v))\n",
    "\n",
    "  def _mean(self):\n",
    "    return tf.broadcast_to(\n",
    "        self.loc + 1 / self.rate, self._batch_shape_tensor())\n",
    "\n",
    "  def _variance(self):\n",
    "    return tf.broadcast_to(\n",
    "        tf.square(self.scale) + 1 / tf.square(self.rate),\n",
    "        self._batch_shape_tensor())\n",
    "\n",
    "  def _parameter_control_dependencies(self, is_init):\n",
    "    assertions = []\n",
    "\n",
    "    if is_init:\n",
    "      try:\n",
    "        self._batch_shape()\n",
    "      except ValueError:\n",
    "        raise ValueError(\n",
    "            'Arguments `loc`, `scale`, and `rate` must have compatible shapes; '\n",
    "            'loc.shape={}, scale.shape={}, rate.shape={}.'.format(\n",
    "                self.loc.shape, self.scale.shape, self.rate.shape))\n",
    "      # We don't bother checking the shapes in the dynamic case because\n",
    "      # all member functions access both arguments anyway.\n",
    "\n",
    "    if not self.validate_args:\n",
    "      assert not assertions  # Should never happen.\n",
    "      return []\n",
    "\n",
    "    if is_init != tensor_util.is_ref(self.scale):\n",
    "      assertions.append(assert_util.assert_positive(\n",
    "          self.scale, message='Argument `scale` must be positive.'))\n",
    "\n",
    "    if is_init != tensor_util.is_ref(self.rate):\n",
    "      assertions.append(assert_util.assert_positive(\n",
    "          self.rate, message='Argument `rate` must be positive.'))\n",
    "\n",
    "    return assertions\n",
    "\n",
    "  def _default_event_space_bijector(self):\n",
    "    return identity_bijector.Identity(validate_args=self.validate_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "VT1C8hFhyYwF"
   },
   "outputs": [],
   "source": [
    "# @title Model: Random Feature.\n",
    "\n",
    "def make_random_feature(X: tf.Tensor, \n",
    "                        lengthscale: float = 1., \n",
    "                        hidden_units: int = 512, \n",
    "                        seed: int = 0, \n",
    "                        return_Wb: bool = False, \n",
    "                        **unused_kwargs) -> Union[tf.Tensor, Tuple[tf.Tensor]]:\n",
    "  \"\"\"Makes random feature for scalable RBF kernel approximation.\n",
    "\n",
    "  Given data point x with shape (P, 1), the random feature is computed as \n",
    "  \n",
    "              Phi(x) = sqrt(2/M) * cos(Wx + b), where\n",
    "\n",
    "  W and b are untrainable random weights (shape [M, P]) and biases \n",
    "  (shape [M, 1]) initialized from Gaussian and uniform distribution, \n",
    "  respectively. Here M is the number of hidden units.\n",
    "  \n",
    "  Args:\n",
    "    X: The input data of shape (batch_size, feature_dim).\n",
    "    lengthscale: The length scale of RBF kernel.\n",
    "    hidden_units: The number of hidden units used to approximate the exact \n",
    "      Gaussian process posterior. Higher number leads to more exact \n",
    "      approximation, although it doesn't necessarily leads to better prediction\n",
    "      performance.\n",
    "    seed: The random seed used to generate the random features.\n",
    "    return_Wb: Whether to return the weights and bias (W, b) of the random \n",
    "      feature mapping cos().\n",
    "    unused_kwargs: Unused keyword arguments for signiture consistency with other\n",
    "      prior functions.\n",
    "\n",
    "  Returns:\n",
    "    Random feature Phi(X) with shape (batch_size, hidden_units). Or \n",
    "    the random weights and biases if return_Wb=True.\n",
    "  \"\"\"\n",
    "  del unused_kwargs\n",
    "\n",
    "  # Configure shapes.\n",
    "  feature_dim = tf.shape(X)[1]\n",
    "  W_shape = (feature_dim, hidden_units)\n",
    "  b_shape = (hidden_units,)\n",
    "\n",
    "  # Apply lengthscale.\n",
    "  X = tf.convert_to_tensor(X, dtype=dtype)\n",
    "  lengthscale = tf.convert_to_tensor(lengthscale, dtype=dtype)\n",
    "  \n",
    "  X = X / lengthscale\n",
    "\n",
    "  # Sample random features.\n",
    "  tf.random.set_seed(seed)\n",
    "  W_dist = ed.initializers.OrthogonalRandomFeatures(stddev=1.)\n",
    "  b_dist = tf.initializers.RandomUniform(0, 2*np.pi)\n",
    "  W = W_dist(W_shape).numpy()\n",
    "  b = b_dist(b_shape).numpy()\n",
    "\n",
    "  multiplier = tf.sqrt(2./hidden_units)\n",
    "  random_feature = multiplier * tf.math.cos(tf.matmul(X, W) + b)\n",
    "  \n",
    "  if return_Wb:\n",
    "    return W, b\n",
    "  return random_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "pin-nFBuzIzs"
   },
   "outputs": [],
   "source": [
    "# @title Model: Gaussian Process.\n",
    "\n",
    "def rfgp_dist(inputs: tf.Tensor,\n",
    "              units: int = 1,\n",
    "              hidden_units: int = 128,               \n",
    "              lengthscale: float = 1., \n",
    "              l2_regularizer: float = 1., \n",
    "              y_noise_std: float = 0.1,\n",
    "              seed: int = 0,\n",
    "              posterior_mode: bool = False,\n",
    "              posterior_sample: Optional[tf.Tensor] = None,\n",
    "              return_joint_dist: bool = True,\n",
    "              verbose: bool = False\n",
    "              ) -> Tuple[Union[tfd.Distribution, List[tfd.Distribution]], \n",
    "                         Dict[str, Any]]:\n",
    "  \"\"\"Specifies a random-feature-based Gaussian process (RFGP) distribution.\n",
    "  \n",
    "  Given random feature mapping Phi(x), a Gaussian process prior \n",
    "  f ~ GaussianProcess can be expressed as:\n",
    "\n",
    "      f(x) ~ matmul(Phi(x), W);   W i.i.d. ~ Normal(0, s)\n",
    "\n",
    "  where W is a i.i.d. Normal prior with shape (num_hidden_units, units), and \n",
    "  its standard deviation s serves as a regularizer to control the smoothness of\n",
    "  f(x) (similar to the l2 penalty in Ridge regression).\n",
    "\n",
    "  This function also allow user to specify a Gaussian process regression model\n",
    "  (by setting y_noise_std>0), that is, it will specify a prior model like below:\n",
    "\n",
    "      y | f ~ N(f(x), y_noise_std);   f ~ GaussianProcess (as specified above)\n",
    "\n",
    "  Args:\n",
    "    inputs: Input data of size (batch_size, feature_dim).\n",
    "    units: Number of outputs from the Gaussian process.\n",
    "    hidden_units: The number of hidden units used to approximate the exact \n",
    "      Gaussian process posterior. Higher number leads to more exact \n",
    "      approximation, although it doesn't necessarily leads to better prediction\n",
    "      performance.\n",
    "    lengthscale: The length scale of RBF kernel.\n",
    "    l2_regularizer: The L2 regularizer to Gaussian process prior, which is \n",
    "      effectively the inverse standard deviation of the prior distribution \n",
    "      of beta.\n",
    "    y_noise_std: The standard deviation of the output y given f. If -1 then \n",
    "      y | f is a deterministic distribution (i.e., y = f).\n",
    "    seed: The random seed used to generate the random features.\n",
    "    posterior_mode: Whether to specify the posterior distribution of the \n",
    "      Gaussian process instead of the prior distribution. If `True`, then \n",
    "      the distribution of `beta` is specified as a sampler from the posterior \n",
    "      sample.\n",
    "    posterior_sample: The posterior sample of `beta` with dimension \n",
    "      (num_mcmc_sample, hidden_units, units) or \n",
    "      (hidden_units, units, num_mcmc_sample). Must be provided if `posterior_mode=True`.\n",
    "    return_joint_dist: If True, return a tfd.Distribution object which \n",
    "      represents the joint distribution of the model random variables [W, y], \n",
    "      which can be used for MCMC sampling. If False, return a list of random\n",
    "      variables which can be used as a building block for more complex models,\n",
    "      and also add the model random features Phi(X) to the model_config.\n",
    "\n",
    "  Returns:\n",
    "    joint_dist: The random variables in the Gaussian process prior [W, y].\n",
    "    model_config: A dictionary of keyword arguments to make_random_feature.\n",
    "  \"\"\"\n",
    "  feature_dim = tf.shape(inputs)[1]\n",
    "  # Latent random features.\n",
    "  random_features = make_random_feature(inputs, \n",
    "                                        lengthscale=lengthscale, \n",
    "                                        hidden_units=hidden_units, \n",
    "                                        seed=seed)\n",
    "    \n",
    "  # Parameter Distributions.\n",
    "  W_dist = tfd.Normal(loc=tf.zeros(shape=(hidden_units, units)), \n",
    "                      scale=1./l2_regularizer)\n",
    "  if posterior_mode:\n",
    "    # If in posterior mode, defines W_dist as the Empirical distribution \n",
    "    # constructed from the posterior samples.\n",
    "    if (posterior_sample.shape[1] == hidden_units and \n",
    "        posterior_sample.shape[2] == units):\n",
    "      # Convert the posterior_sample shape from  \n",
    "      # (num_sample, hidden_units, output_units) to\n",
    "      # (hidden_units, output_units, num_sample).\n",
    "      posterior_sample = tf.transpose(posterior_sample, [1, 2, 0])\n",
    "\n",
    "    sample_hidden_dim = posterior_sample.shape[0]\n",
    "    sample_output_dim = posterior_sample.shape[1]\n",
    "    num_sample = posterior_sample.shape[2]\n",
    "\n",
    "    if (sample_hidden_dim != hidden_units or \n",
    "        sample_output_dim != units or \n",
    "        len(posterior_sample.shape) != 3):\n",
    "      raise ValueError(\n",
    "          \"Expect posterior_sample shape to be \"\n",
    "          f\"[num_sample, hidden_units({hidden_units}),units({units})] or \"\n",
    "          f\"[hidden_units({hidden_units}),units({units}), num_sample]. Got \"\n",
    "          f\"{posterior_sample.shape}.\")\n",
    "\n",
    "    if verbose:\n",
    "      print(f\"Constructing posterior from {num_sample} samples.\")\n",
    "    W_dist = tfd.Empirical(posterior_sample, event_ndims=0)\n",
    "  \n",
    "  # Outcome Distributions.\n",
    "  if y_noise_std > 0.:\n",
    "    y_dist = lambda gp_weights: tfd.Normal(\n",
    "        loc=tf.linalg.matmul(random_features, gp_weights), scale=y_noise_std)\n",
    "  else:\n",
    "    y_dist = lambda gp_weights: tfd.Deterministic(\n",
    "        loc=tf.linalg.matmul(random_features, gp_weights))\n",
    "        \n",
    "  joint_dist = dict(gp_weights=W_dist, y=y_dist)\n",
    "\n",
    "  if return_joint_dist:\n",
    "    joint_dist = tfd.JointDistributionNamedAutoBatched(joint_dist)\n",
    "\n",
    "  # Model config.\n",
    "  model_config = dict(units=units,\n",
    "                      hidden_units=hidden_units,                             \n",
    "                      lengthscale=lengthscale, \n",
    "                      l2_regularizer=l2_regularizer, \n",
    "                      y_noise_std=y_noise_std,\n",
    "                      seed=seed)\n",
    "  \n",
    "  if not return_joint_dist:\n",
    "    model_config['random_features'] = random_features\n",
    "\n",
    "  return joint_dist, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "2LHTumLHCF1K"
   },
   "outputs": [],
   "source": [
    "# @title Model: Bayesian Model Averaging.\n",
    "def bma_dist(inputs: tf.Tensor, \n",
    "             base_model_preds: tf.Tensor, \n",
    "             y_noise_std: float = 0.1,\n",
    "             posterior_mode: bool = False,\n",
    "             posterior_sample: Optional[tf.Tensor] = None,\n",
    "             sample_intermediate_variables: bool = False,\n",
    "             return_joint_dist: bool = True,\n",
    "             debug_mode: bool = False,\n",
    "             **gp_kwargs):\n",
    "  \"\"\"Specifies an adaptive Bayesian model averaging (BMA) model.\n",
    "  \n",
    "  This function specifies an adaptive ensemble model with model weights \n",
    "  parameterized by Gaussian processes:\n",
    "\n",
    "      y | f ~ N(m(x), y_noise_std)  where  m = sum_k p_k(x) * m_k(x),       \n",
    "      \n",
    "  where m_k(x) are base model predictions, and p_k's are adaptive ensemble \n",
    "  weights specified as:\n",
    "\n",
    "      [p_1(x), ..., p_K(x)] = softmax([f_1(x), ..., f_K(x)])\n",
    "      [f_1, ..., f_K]  ~ i.i.d.  GaussianProcess\n",
    "\n",
    "  Args:\n",
    "    inputs: Input data of size (batch_size, feature_dim).\n",
    "    base_model_preds: Base model predictions of shape (batch_size, num_base_models).\n",
    "    y_noise_std: The standard deviation of the output distribution y | f.\n",
    "    posterior_mode: Whether to specify the posterior distribution of the \n",
    "      Gaussian process instead of the prior distribution. If `True`, then \n",
    "      the distribution of `beta` is specified as a sampler from the posterior \n",
    "      sample.\n",
    "    posterior_sample: The posterior sample of `beta` with dimension \n",
    "      (hidden_units, units, num_mcmc_sample). Must be provided if \n",
    "      `posterior_mode=True`.\n",
    "    sample_intermediate_variables: Whether to generate posterior samples of\n",
    "      intermediate variables for model understanding when `posterior_mode=True`.\n",
    "    return_joint_dist: If True, return a tfd.Distribution object which \n",
    "      represents the joint distribution of the model random variables [W, y], \n",
    "      which can be used for MCMC sampling. If False, return a list of random\n",
    "      variables which can be used as a building block for more complex models,\n",
    "      and also add the model random features Phi(X) to the model_config.\n",
    "\n",
    "  Returns:\n",
    "    joint_dist: The random variables in the Gaussian process prior [W, y].\n",
    "    model_config: A dictionary of keyword arguments to make_random_feature.\n",
    "  \"\"\"\n",
    "  num_model_data, num_model = base_model_preds.shape\n",
    "  num_input_data, num_input_features = inputs.shape\n",
    "\n",
    "  if num_model_data != num_input_data:\n",
    "    raise ValueError(\n",
    "        f'Number of data points in input data ({num_input_data}) and in model '\n",
    "        f'predictions ({num_model_data}) should be equal.')\n",
    "\n",
    "  # Specifies Gaussian process priors for ensemble weights.\n",
    "  gp_kwargs.pop('y_noise_std', None)\n",
    "  gp_kwargs.pop('units', None)\n",
    "  gp_dists, gp_config = rfgp_dist(\n",
    "      inputs, \n",
    "      units=num_model, \n",
    "      posterior_mode=posterior_mode,\n",
    "      posterior_sample=posterior_sample,\n",
    "      return_joint_dist=False,\n",
    "      verbose=debug_mode,\n",
    "      **gp_kwargs)\n",
    "\n",
    "  gp_features = gp_config.pop('random_features')\n",
    "  gp_weight_dist = gp_dists[\"gp_weights\"]  \n",
    "\n",
    "  # Specifies Bayesian model averaging model.\n",
    "  joint_dist = dict()\n",
    "\n",
    "  joint_dist[\"gp_weights\"] = gp_weight_dist\n",
    "  if posterior_mode and sample_intermediate_variables:\n",
    "    # Generates posterior of all intermediate variables.\n",
    "    joint_dist[\"gps\"] = lambda gp_weights: tfd.Deterministic(\n",
    "        loc=build_ensemble_weight_logits(gp_weights, gp_features))\n",
    "    joint_dist[\"ensemble_weights\"] = lambda gps: tfd.Deterministic(\n",
    "        loc=build_ensemble_weights(gps))\n",
    "    joint_dist[\"y\"] = lambda ensemble_weights: tfd.Normal(\n",
    "        loc=ensemble_prediction(ensemble_weights, base_model_preds), \n",
    "        scale=y_noise_std)\n",
    "  else:\n",
    "    # Use collapsed joint distribution for easy MCMC sampling.\n",
    "    joint_dist[\"y\"] = lambda gp_weights: tfd.Normal(\n",
    "        loc=ensemble_mean(gp_weights, base_model_preds, gp_features), \n",
    "        scale=y_noise_std)\n",
    "  \n",
    "  if return_joint_dist:\n",
    "    joint_dist = tfd.JointDistributionNamedAutoBatched(joint_dist) \n",
    "  \n",
    "  # Store model config.\n",
    "  model_config = gp_config\n",
    "  model_config['y_noise_std']=y_noise_std\n",
    "\n",
    "  return joint_dist, model_config\n",
    "  \n",
    "\n",
    "# Utility functions.\n",
    "def build_ensemble_weight_logits(gp_weights, gp_features):\n",
    "  \"\"\"Builds Gaussian process prediction from random-feature weights.\"\"\"\n",
    "  return tf.linalg.matmul(gp_features, gp_weights) \n",
    "\n",
    "def build_ensemble_weights(logits):\n",
    "  \"\"\"Builds ensemble weights from Gaussian process prediction.\"\"\"\n",
    "  return tf.keras.activations.softmax(logits)\n",
    "\n",
    "def ensemble_prediction(weights, base_model_preds):\n",
    "  \"\"\"Builds final ensemble prediction from ensemble weights and base models.\"\"\"\n",
    "  return tf.reduce_sum(base_model_preds * weights, axis=-1, keepdims=True)\n",
    "\n",
    "def ensemble_mean(gp_weights, base_model_preds, gp_features):\n",
    "  \"\"\"Computes final ensemble prediction directly from random-feature weights\"\"\"\n",
    "  logits = build_ensemble_weight_logits(gp_weights, gp_features)\n",
    "  ensemble_weights = build_ensemble_weights(logits)\n",
    "  return ensemble_prediction(ensemble_weights, base_model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "uhJ0m8avieCL"
   },
   "outputs": [],
   "source": [
    "# @title Model: Bayesian Nonparametric Ensemble\n",
    "def bne_model_dist(inputs: tf.Tensor, \n",
    "                   mean_preds: Union[float, tf.Tensor],\n",
    "                   estimate_mean=True,\n",
    "                   estimate_variance=True,\n",
    "                   estimate_skewness=True,\n",
    "                   variance_prior_mean=0.,\n",
    "                   skewness_prior_mean=0.,\n",
    "                   posterior_mode: bool = False,\n",
    "                   posterior_sample: Optional[tf.Tensor] = None,\n",
    "                   return_joint_dist: bool = True,\n",
    "                   sample_intermediate_variables: bool = False,\n",
    "                   debug_mode: bool = False,\n",
    "                   **gp_kwargs):\n",
    "  \"\"\"BNE model with spatially adaptive variances and skewness.\n",
    "\n",
    "  Given the original prediction m(x) from a Bayesian model, the Bayesian \n",
    "  nonparametric ensemble (BNE) improves the original model's uncertainty \n",
    "  quality by deploying a semi-parametric distribution estimator that flexibly\n",
    "  captures the higher moments of the data distribution (i.e., the aleatoric \n",
    "  uncertainty) while also quantifying model's epistemic uncertainty (due to the \n",
    "  lack of data).\n",
    "\n",
    "  Specifically, we assume:\n",
    "\n",
    "  y ~ ExponentiallyModifiedGaussian(m(x) + loc(x), scale(x), rate(x)) \n",
    "\n",
    "  where loc, scale, rate are (transformed) Gaussian processes that adaptively \n",
    "  estimates higher moments of the data distribution:\n",
    "\n",
    "    loc         ~ GaussianProcess(0, k_loc)\n",
    "    log(scale)  ~ GaussianProcess(0, k_scale)\n",
    "    log(rate)   ~ GaussianProcess(0, k_rate)   \n",
    "    \n",
    "  \n",
    "  Args:\n",
    "    inputs: Input data of size (batch_size, feature_dim).\n",
    "    mean_preds: Original mean predictions. Either a scalar or an array of shape \n",
    "      (batch_size, 1).\n",
    "    y_noise_std: The standard deviation of the output distribution y | f.\n",
    "    estimate_mean: Whether to add residual process to model.\n",
    "    estimate_variance: Whether to estimate spatio-temporally adaptive variance \n",
    "      of the outcome distribution using Gaussian process.\n",
    "    estimate_skewness: Whether to estimate spatio-temporally adaptive skewness \n",
    "      of the outcome distribution using Gaussian process.\n",
    "    skewness_prior_mean: The prior mean for the Gaussian process logits for the \n",
    "      skewness parameter. If user which the model to not estimate the skewness,\n",
    "      can set this to a very large negative value. e.g., -25.0.\n",
    "    posterior_mode: Whether to specify the posterior distribution of the \n",
    "      Gaussian process instead of the prior distribution. If `True`, then \n",
    "      the distribution of `beta` is specified as a sampler from the posterior \n",
    "      sample.\n",
    "    posterior_sample: The posterior sample of `beta` with dimension \n",
    "      (hidden_units, units, num_mcmc_sample). Must be provided if \n",
    "      `posterior_mode=True`.\n",
    "    return_joint_dist: If True, return a tfd.Distribution object which \n",
    "      represents the joint distribution of the model random variables [W, y], \n",
    "      which can be used for MCMC sampling. If False, return a list of random\n",
    "      variables which can be used as a building block for more complex models,\n",
    "      and also add the model random features Phi(X) to the model_config.\n",
    "    sample_intermediate_variables: Whether to generate posterior samples of\n",
    "      intermediate variables for model understanding when `posterior_mode=True`.\n",
    "    **gp_kwargs: Optional keyword arguments to the rfgp_dist() which generates \n",
    "      the Gaussian processes.\n",
    "\n",
    "  Returns:\n",
    "    joint_dist: The random variables in the Gaussian process prior [W, y].\n",
    "    model_config: A dictionary of keyword arguments to make_random_feature.  \n",
    "  \"\"\"\n",
    "  num_input_data, num_input_features = inputs.shape\n",
    "\n",
    "  # Specify GP priors.\n",
    "  gp_kwargs.pop('y_noise_std', None)\n",
    "  gp_kwargs.pop('units', None)\n",
    "\n",
    "  bne_units = 2 + int(estimate_skewness)\n",
    "  bne_gp_dists, bne_gp_config = rfgp_dist(\n",
    "      inputs, \n",
    "      # Perform inference for mean, optionlly variance and skewness.      \n",
    "      units=bne_units,  \n",
    "      y_noise_std=-1.,  \n",
    "      posterior_mode=posterior_mode,\n",
    "      posterior_sample=posterior_sample,\n",
    "      return_joint_dist=False,\n",
    "      verbose=debug_mode,\n",
    "      **gp_kwargs)\n",
    "\n",
    "  gp_weight_dist = bne_gp_dists[\"gp_weights\"]\n",
    "  gp_features = bne_gp_config.pop('random_features')\n",
    "\n",
    "  if debug_mode:\n",
    "    print(f'estimate_mean: {estimate_mean}')\n",
    "    print(f'estimate_variance: {estimate_variance}')\n",
    "    print(f'estimate_skewness: {estimate_skewness}')\n",
    "\n",
    "  # Specify mean distribution.\n",
    "  if estimate_mean:\n",
    "    def mean_from_gps(gps):\n",
    "      return gps[:, 0, tf.newaxis]\n",
    "  else:\n",
    "    def mean_from_gps(gps):\n",
    "      return tf.zeros_like(gps)[:, 0, tf.newaxis]\n",
    "\n",
    "  def mean_dist_from_gps(gps):\n",
    "    return tfd.Deterministic(loc=mean_from_gps(gps))\n",
    "\n",
    "  # Specify variance distribution.  \n",
    "  if estimate_variance:\n",
    "    # Model variance as a log Gaussian process.\n",
    "    def scale_from_gps(gps, gp_weights):\n",
    "      return tf.exp(gps[:, 1, tf.newaxis] + variance_prior_mean)\n",
    "  else:\n",
    "    # Model variance as a scalar (by taking an element from GP weights).\n",
    "    def scale_from_gps(gps, gp_weights):\n",
    "      return tf.exp(gp_weights[0, 1] + variance_prior_mean)\n",
    "      \n",
    "  def scale_dist_from_gps(gps, gp_weights):\n",
    "    return tfd.Deterministic(loc=scale_from_gps(gps, gp_weights))\n",
    "\n",
    "  # Specify skewness distribution.\n",
    "  if estimate_skewness:\n",
    "    # Estimate mean, variance and skewness using EMG distribution.\n",
    "    def rate_from_gps(gps):\n",
    "      return tf.exp(gps[:, 2, tf.newaxis] + skewness_prior_mean)\n",
    "\n",
    "    def y_dist_from_moments(mean, scale, rate):\n",
    "      return ExponentiallyModifiedGaussian(\n",
    "          loc=mean_preds + mean, scale=scale, rate=rate)\n",
    "\n",
    "    def y_dist_from_weights(gp_weights):\n",
    "      gps = tf.matmul(gp_features, gp_weights)\n",
    "      mean = mean_from_gps(gps)\n",
    "      scale = scale_from_gps(gps, gp_weights)\n",
    "      rate = rate_from_gps(gps)\n",
    "      return ExponentiallyModifiedGaussian(\n",
    "          loc=mean_preds + mean, scale=scale, rate=rate)\n",
    "  else:\n",
    "    # Estimate mean and variance using Gaussian distribution.\n",
    "    def y_dist_from_moments(mean, scale):\n",
    "      return tfd.Normal(loc=mean_preds + mean, scale=scale)\n",
    "\n",
    "    def y_dist_from_weights(gp_weights):\n",
    "      gps = tf.matmul(gp_features, gp_weights)\n",
    "      mean = mean_from_gps(gps)\n",
    "      scale = scale_from_gps(gps, gp_weights)\n",
    "      return tfd.Normal(loc=mean_preds + mean, scale=scale)\n",
    "  \n",
    "  # Specify full joint distributions.\n",
    "  joint_dist = dict()\n",
    "  if posterior_mode and sample_intermediate_variables:\n",
    "    joint_dist['gp_weights'] = gp_weight_dist\n",
    "    joint_dist['gps'] = lambda gp_weights: tfd.Deterministic(\n",
    "        loc=tf.matmul(gp_features, gp_weights))\n",
    "    joint_dist['mean'] = mean_dist_from_gps\n",
    "    joint_dist['scale'] = scale_dist_from_gps\n",
    "    if estimate_skewness:\n",
    "      joint_dist['rate'] = lambda gps: tfd.Deterministic(\n",
    "          loc=rate_from_gps(gps))\n",
    "      joint_dist['skewness'] = lambda rate: tfd.Exponential(rate=rate)\n",
    "    joint_dist['y'] = y_dist_from_moments\n",
    "  else: \n",
    "    joint_dist['gp_weights'] = gp_weight_dist\n",
    "    joint_dist['y'] = y_dist_from_weights\n",
    "\n",
    "  if return_joint_dist:\n",
    "    joint_dist = tfd.JointDistributionNamedAutoBatched(joint_dist)\n",
    "  \n",
    "  return joint_dist, bne_gp_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "2ZK83EyTzCNQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\nickl\\AppData\\Local\\Temp\\ipykernel_9752\\93144848.py:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if kernel_type is 'hmc':\n",
      "C:\\Users\\nickl\\AppData\\Local\\Temp\\ipykernel_9752\\93144848.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if kernel_type is 'hmc':\n",
      "C:\\Users\\nickl\\AppData\\Local\\Temp\\ipykernel_9752\\93144848.py:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if step_adaptor_type is 'simple':\n"
     ]
    }
   ],
   "source": [
    "# @title MCMC: Low-level sampler.\n",
    "\n",
    "@tf.function(autograph=False)\n",
    "def run_chain(init_state: List[tf.Tensor], \n",
    "              step_size: float, \n",
    "              target_log_prob_fn: Callable[..., tf.Tensor], \n",
    "              num_steps: int = 500, \n",
    "              burnin: int = 100, \n",
    "              seed: int = 0,\n",
    "              kernel_type: str = \"hmc\",\n",
    "              step_adaptor_type: str = \"simple\"\n",
    "              ) -> Union[List[tf.Tensor], Tuple[tf.Tensor]]:\n",
    "  \"\"\"Low-level function that runs MCMC sampling for a given model posterior.\n",
    "  \n",
    "  Args:\n",
    "    init_state: The initial state for the MCMC sampler.\n",
    "    step_size: The step size of a Hamiltonian Monte Carlo step.\n",
    "    target_log_prob_fn: The log likelihood function for model posterior.\n",
    "    num_steps: The number of total MCMC samples to return.\n",
    "    burnin: The length of the burn-in period for MCMC warmup.\n",
    "    seed: The random seed for MCMC sampling.\n",
    "    kernel_type: Type of MCMC kernel to use, either ('hmc', 'nuts').\n",
    "    step_adaptor_type: Type of MCMC kernel to use, one of \n",
    "      ('simple', 'dual_averaging').\n",
    "\n",
    "  Returns:\n",
    "    chain_state: Posterior sample from all MCMC chains.\n",
    "    sampler stat: Sampling statistics, currently (step_size, acceptance ratio).\n",
    "  \"\"\"\n",
    "  if kernel_type not in ('hmc', 'nuts'):\n",
    "    raise ValueError(\n",
    "        f\"kernel_type {kernel_type} must be one of ('hmc', 'nuts').\")\n",
    "\n",
    "  if step_adaptor_type not in ('simple', 'dual_averaging'):\n",
    "    raise ValueError(\n",
    "        f\"step_adaptor_type {step_adaptor_type} must be one of \"\n",
    "        \"('simple', 'dual_averaging').\")\n",
    "\n",
    "  def trace_fn(_, pkr): \n",
    "    if kernel_type is 'hmc':\n",
    "      step_size = pkr.inner_results.accepted_results.step_size\n",
    "    else:\n",
    "      step_size = pkr.inner_results.step_size\n",
    "\n",
    "    return (step_size, pkr.inner_results.log_accept_ratio)\n",
    "\n",
    "  if kernel_type is 'hmc':\n",
    "    kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=target_log_prob_fn,\n",
    "        num_leapfrog_steps=5,\n",
    "        step_size=step_size)\n",
    "    step_adaptation_kwargs = dict()\n",
    "  else:\n",
    "    kernel = tfp.mcmc.NoUTurnSampler(\n",
    "        target_log_prob_fn=target_log_prob_fn,\n",
    "        step_size=step_size)\n",
    "    step_adaptation_kwargs = dict(\n",
    "        step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(\n",
    "            step_size=new_step_size),\n",
    "        step_size_getter_fn=lambda pkr: pkr.step_size,\n",
    "        log_accept_prob_getter_fn=lambda pkr: pkr.log_accept_ratio,)\n",
    "\n",
    "  if step_adaptor_type is 'simple':\n",
    "    kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "      inner_kernel=kernel, \n",
    "      num_adaptation_steps=burnin)\n",
    "  else:\n",
    "    kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
    "      inner_kernel=kernel,\n",
    "      num_adaptation_steps=burnin,\n",
    "      target_accept_prob=0.75,\n",
    "      **step_adaptation_kwargs)\n",
    "\n",
    "  # Execute sampling.\n",
    "  chain_state, sampler_stat = tfp.mcmc.sample_chain(\n",
    "      num_results=num_steps,\n",
    "      num_burnin_steps=burnin,\n",
    "      current_state=init_state,\n",
    "      kernel=kernel,\n",
    "      trace_fn=trace_fn,\n",
    "      seed=seed)\n",
    "    \n",
    "  return chain_state, sampler_stat\n",
    "\n",
    "def mix_chain_samples(samples: Union[tf.Tensor, List[tf.Tensor]]):\n",
    "  \"\"\"Mix MCMC samplers from different chains.\n",
    "  \n",
    "    Given a posterior sample with shape [num_sample, num_chain, ...], \n",
    "    collapse the samples from different chains by reshaping it as \n",
    "    [..., num_sample * num_chain].\n",
    "\n",
    "  Args:\n",
    "    samples: The posterior sample from multiple chains.\n",
    "\n",
    "  Returns:\n",
    "    The collapsed posterior samples.\n",
    "  \"\"\"\n",
    "  if not isinstance(samples, list):\n",
    "    samples = [samples]\n",
    "\n",
    "  mixed_samples = []\n",
    "  for sample in samples:\n",
    "    sample_shape = list(tf.shape(sample).numpy())\n",
    "    sample = tf.reshape(sample, [-1] + sample_shape[2:])\n",
    "    sample = tf.transpose(sample, [1, 2, 0])\n",
    "\n",
    "    mixed_samples.append(sample)\n",
    "\n",
    "  return mixed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "hK6uAJ0yuKKK"
   },
   "outputs": [],
   "source": [
    "# @title MCMC: Main API.\n",
    "\n",
    "def run_mcmc(init_state: Optional[List[tf.Tensor]] = None,\n",
    "             target_log_prob_fn: Optional[Callable[..., tf.Tensor]] = None, \n",
    "             model_dist: Optional[List[tfd.Distribution]] = None,      \n",
    "             y: Optional[tf.Tensor] = None,\n",
    "             sample_size: int = 500, \n",
    "             nchain: int = 10,             \n",
    "             num_steps: int = 500, \n",
    "             burnin: int = 100, \n",
    "             step_size: float = .1, \n",
    "             seed: int = 0, \n",
    "             debug_mode: bool = False,\n",
    "             **mcmc_kwargs):\n",
    "  \"\"\"Executes MCMC training for a given model posterior.\n",
    "  \n",
    "  Args:\n",
    "    target_log_prob_fn: The log likelihood function of modle posterior.\n",
    "      If not provided, then a default set of (init_state, target_log_prob_fn)\n",
    "      will be generated by `prepare_mcmc`.    \n",
    "    init_state: The initial states to the MCMC sampler, a list of tf.tensors \n",
    "      with shape (num_chains, num_variables). If not provided, then a default \n",
    "      set of (init_state, target_log_prob_fn) will be generated by \n",
    "      `prepare_mcmc`.\n",
    "    model_dist: The model posterior distribution to be used by `prepare_mcmc`. \n",
    "      Must be provided if init_state or target_log_prob_fn is None.\n",
    "    y: The output variable with shape (batch_size, ) to be used by \n",
    "      `prepare_mcmc`.  Must be provided if init_state or target_log_prob_fn is\n",
    "       None.\n",
    "    sample_size: The number of the final MCMC samples to return after thinning\n",
    "      the gathered MCMC samples.\n",
    "    n_chain: The number of MCMC chain in sampling.\n",
    "    num_steps: The number of total MCMC samples to generate.\n",
    "    burnin: The length of the burn-in period for MCMC warmup.\n",
    "    seed: The random seed for MCMC sampling.\n",
    "    debug_mode: If True. also return the original unmixed samples.\n",
    "    **mcmc_kwargs: Additional keyword arguments to pass to the low-level MCMC\n",
    "      function.\n",
    "\n",
    "  Return:\n",
    "    mixed_samples: A list of posterior samples with shapes [sample_size, ...]\n",
    "      for each variable in the model posterior. \n",
    "    sampler_stat: diagnostic statistics of the MCMC chain, which contains \n",
    "      the step size and the proposal acceptance of each HMC step.\n",
    "  \"\"\"\n",
    "  # Prepares initial states and model log likelihoods for MCMC.\n",
    "  if init_state is None or target_log_prob_fn is None:\n",
    "    # By default, sample first parameter of a two-parameter model (W, y).\n",
    "    init_state, target_log_prob_fn = prepare_mcmc(\n",
    "        model_dist, y, nchain=nchain)\n",
    "    \n",
    "  # Perform MCMC.\n",
    "  chain_samples, sampler_stat = run_chain(\n",
    "      init_state=init_state, \n",
    "      step_size=step_size,\n",
    "      target_log_prob_fn=target_log_prob_fn,\n",
    "      num_steps=num_steps, \n",
    "      burnin=burnin, \n",
    "      seed=seed,\n",
    "      **mcmc_kwargs)\n",
    "  # Clear tf.function cache.\n",
    "  try:\n",
    "    run_chain._stateful_fn._function_cache.clear()\n",
    "  except:\n",
    "    run_chain._stateful_fn._function_cache.primary.clear()\n",
    "\n",
    "  # Thining.\n",
    "  sample_size_per_chain = int(sample_size / nchain)\n",
    "  sample_ids = np.linspace(\n",
    "      0, num_steps-1, sample_size_per_chain).astype(int)\n",
    "  chain_samples_thinned = chain_samples.numpy()[sample_ids]\n",
    "\n",
    "  # Mix examples from different chains, \n",
    "  # Shape [param_dim_1, param_dim_2, num_mcmc_samples].\n",
    "  mixed_samples = mix_chain_samples(chain_samples_thinned)\n",
    "\n",
    "  # Check acceptance probability.\n",
    "  p_accept = tf.math.exp(tfp.math.reduce_logmeanexp(\n",
    "    tf.minimum(sampler_stat[-1], 0.)))\n",
    "  print(f'Acceptance Ratio: {p_accept}')\n",
    "  \n",
    "  if debug_mode:\n",
    "    return mixed_samples, chain_samples, sampler_stat\n",
    "  return mixed_samples, sampler_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "_EBMjVZ_D29k"
   },
   "outputs": [],
   "source": [
    "# @title MAP: Main API.\n",
    "\n",
    "def run_map(target_log_prob_fn, gp_config,\n",
    "            learning_rate=0.1, \n",
    "            num_steps=20_000, print_every=1_000, seed=None):\n",
    "  \"\"\"Executes MAP estimation using Adam optimizer.\"\"\"\n",
    "  # Prepares input variable.\n",
    "  W_shape = (gp_config['hidden_units'], gp_config['units'])\n",
    "  W_init = tf.initializers.HeNormal(seed=seed)(shape=W_shape)\n",
    "  W_var = tf.Variable(initial_value=W_init)\n",
    "\n",
    "  # Prepares loss function (negative log-likelihood).\n",
    "  @tf.function\n",
    "  def loss():\n",
    "    nll = -target_log_prob_fn(W_var)\n",
    "    return tf.reduce_mean(nll)\n",
    "\n",
    "  # Runs optimization loop using Adam optimizer.\n",
    "  opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "  for iter in range(num_steps):\n",
    "    if iter % print_every == 0:\n",
    "      print(f'{loss().numpy()}...', end='')\n",
    "    _ = opt.minimize(loss, [W_var])\n",
    "  print('Done.')\n",
    "\n",
    "  return tf.constant(W_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyVOAW4EODnT"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "ShaDwurNOD_w"
   },
   "outputs": [],
   "source": [
    "# @title Visualization: plot_distribution\n",
    "def plot_distribution(X, Y, loc=None, scale=None, X_train=None, Y_train=None, \n",
    "                      loc_color=\"black\", scale_color=\"silver\",\n",
    "                      alpha_k=(1., 0.6, 0.3), interval_only=False):\n",
    "  if isinstance(X, tuple):\n",
    "    X, _ = X\n",
    "  if isinstance(X_train, tuple):\n",
    "    X_train, _ = X_train    \n",
    "  \n",
    "  x = X.squeeze()\n",
    "  \n",
    "  if not interval_only:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "  if loc is not None and scale is not None:\n",
    "    for k in (3, 2, 1):\n",
    "      lb = (loc - k * scale).squeeze()\n",
    "      ub = (loc + k * scale).squeeze()\n",
    "      plt.fill_between(x, lb, ub, color=scale_color, alpha=alpha_k[3-k])\n",
    "    plt.plot(x, lb, color=scale_color, alpha=0.5)\n",
    "    plt.plot(x, ub, color=scale_color, alpha=0.5)\n",
    "    plt.plot(X, loc, color=loc_color)\n",
    "\n",
    "  if not interval_only:\n",
    "    plt.scatter(X, Y, color=\"gray\", alpha=0.8)\n",
    "    if X_train is not None:\n",
    "      plt.scatter(X_train, Y_train, color=\"black\", alpha=0.6)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpr_2d_visual(pred_mean, pred_cov,\n",
    "                  X_train, y_train, X_test, y_test,\n",
    "                  title=\"\", save_addr=\"\", fontsize=12):\n",
    "#     if save_addr:\n",
    "#         pathlib.Path(save_addr).parent.mkdir(parents=True, exist_ok=True)\n",
    "#         plt.ioff()\n",
    "\n",
    "    # prediction surface\n",
    "    n_reshape = int(np.sqrt(pred_mean.size))\n",
    "    pred_mean_plot = pred_mean.reshape(n_reshape, n_reshape)\n",
    "    X_valid = X_test.reshape(n_reshape, n_reshape, 2)\n",
    "    x_grid, y_grid = X_valid[:, :, 0], X_valid[:, :, 1]\n",
    "\n",
    "    ax = plt.axes(projection='3d')\n",
    "    if isinstance(X_train, np.ndarray):\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c=\"black\")\n",
    "    ax.plot_surface(X=x_grid, Y=y_grid, Z=pred_mean_plot, cmap='inferno')\n",
    "    ax.set_zlim(np.min(y_test), np.max(y_test))\n",
    "\n",
    "#     # optionally, compute RMSE\n",
    "#     if pred_mean.size == y_test.size:\n",
    "#         rmse = metric_util.rmse(y_test, pred_mean)\n",
    "#         title = \"{}, RMSE={:.4f}\".format(title, rmse)\n",
    "\n",
    "    plt.title(title, fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "EkkjY15MZ-g1"
   },
   "outputs": [],
   "source": [
    "# @title Visualization: plot_base_models\n",
    "def plot_base_models(base_preds_test, kernel_names,\n",
    "                     X_test, Y_test, X_train, Y_train, X_base, Y_base, \n",
    "                     ax=plt):\n",
    "  for base_pred in tf.transpose(base_preds_test):\n",
    "    ax.plot(X_test, base_pred)\n",
    "\n",
    "  ax.scatter(X_train, Y_train, c='b', s=10)\n",
    "  ax.scatter(X_base, Y_base, c='r', s=10)\n",
    "  ax.scatter(X_test, Y_test, c='k', alpha=0.1)\n",
    "\n",
    "  ax.legend(kernel_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "HIXMI-WAB2mD"
   },
   "outputs": [],
   "source": [
    "# @title Data: get_base_prediction.\n",
    "def get_base_prediction(X: np.ndarray, Y: np.ndarray, \n",
    "                        X_test: Optional[np.ndarray] = None, \n",
    "                        kernel: Optional[gpf.kernels.Kernel] = None,                         \n",
    "                        model: Optional[gpf.models.BayesianModel] = None, \n",
    "                        num_train_steps: int = 200,\n",
    "                        debug_mode: bool = False) -> Union[gpf.models.GPR, np.ndarray]:\n",
    "  \"\"\"Utility functions to train a base model.\n",
    "  \n",
    "  Args:\n",
    "    X: Training data features.\n",
    "    Y: Training data outcomes.\n",
    "    X_test: Testing data to evaluate the model on. Default to the training data\n",
    "      X.\n",
    "    model: A trained GPFlow model.\n",
    "    kernel: A GPFlow kernel that can be used to define a GPFlow Gaussian process.\n",
    "      Cannot be None if `model = None`.\n",
    "    num_train_steps: The number of gradient steps to optimize the Gaussian \n",
    "      process parameters.\n",
    "\n",
    "  Returns:\n",
    "    A trained GPFlow model if `model=None` and kernel is provided.\n",
    "    Model prediction on X_test if model is provided.  \n",
    "  \"\"\"\n",
    "  # Convert all data to float64 for GPFlow compatibility.\n",
    "  X = X.astype(np.float64)\n",
    "  Y = Y.astype(np.float64)\n",
    "  X_test = X if X_test is None else X_test.astype(np.float64)\n",
    "\n",
    "  if model is None:\n",
    "    # Define model and train with `num_train_steps` number of steps.\n",
    "    if debug_mode:\n",
    "      print(f'Training GP with kernel `{kernel.name}`.')\n",
    "    model = gpf.models.GPR(data=(X, Y), kernel=kernel)\n",
    "    opt = tf.optimizers.Adam(1e-3)\n",
    "    for _ in range(num_train_steps):\n",
    "      opt.minimize(model.training_loss, model.trainable_variables)\n",
    "    return model\n",
    "  else:\n",
    "    # Return prediction.\n",
    "    if debug_mode:\n",
    "      print(f'Predicting GP with kernel \"{model.kernel.name}\".')\n",
    "    prediction = model.predict_f(X_test)[0]\n",
    "    prediction = prediction.numpy().flatten().astype(np.float32)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "_iQl96eHB-s5"
   },
   "outputs": [],
   "source": [
    "# @title Data: process_mcmc_data\n",
    "\n",
    "def process_mcmc_data(mcmc_sample, X, Y, op_type='flatten', debug_mode=False):\n",
    "  \"\"\"Transforms data to / from a MCMC-compatible format.\n",
    "  \n",
    "  Given MCMC samples from a pre-trained model (shape (num_train, ..., num_mcmc)) \n",
    "  and original data (X, Y) (shape [num_train, ...]), transforms them into a MCMC\n",
    "  compatible format. Namely, the mcmc_sample will be reshaped to \n",
    "  (batch_size * num_mcmc, ...), and (X, Y) will be repeated at stacked to \n",
    "  (batch_size * num_mcmc, ...) as well.\n",
    "\n",
    "  Alternatively, this function can also 'deflatten' a mcmc_sample with shape\n",
    "  (num_mcmc_new, num_train * num_mcmc, ...) to \n",
    "  (num_mcmc_new * num_mcmc, num_train, ...).\n",
    "\n",
    "  Args:\n",
    "    mcmc_sample: MCMC samples from a pre-trained model. \n",
    "      If op_type='flatten', shape (num_train, ..., num_mcmc).\n",
    "      If op_type='deflatten', shape (num_mcmc_new, num_train * num_mcmc, ...).\n",
    "    X: Training data features. Shape (num_train, feature_dim) \n",
    "    Y: Training data response, shape (num_train, ...)\n",
    "    op_type: Type of operation to ('flatten', 'deflatten')\n",
    "    \n",
    "  Returns:\n",
    "    If op_type='flatten':\n",
    "\n",
    "    sample_processed: shape (num_train * num_mcmc, ...)\n",
    "    X_processed: shape (num_train * num_mcmc, feature_dim)\n",
    "    Y_processed: shape (num_train * num_mcmc, ...)\n",
    "\n",
    "    If op_type='deflatten':\n",
    "\n",
    "    Reshaped MCMC samples from (num_mcmc_new, num_train * num_mcmc, ...) to\n",
    "    (num_mcmc_new * num_mcmc, num_train, ...).\n",
    "  \"\"\"\n",
    "  mcmc_sample = tf.convert_to_tensor(mcmc_sample, dtype=tf.float32)\n",
    "  X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "  Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "  if op_type == 'flatten':\n",
    "    return flatten_mcmc_data(mcmc_sample, X, Y, debug_mode)\n",
    "  if op_type == 'deflatten':\n",
    "    return deflatten_mcmc_sample(mcmc_sample, X, Y, debug_mode)\n",
    "\n",
    "\n",
    "def flatten_mcmc_data(mcmc_sample, X, Y, debug_mode=False):\n",
    "  \"\"\"Merge input data to shapes (num_train*num_mcmc, ...).\n",
    "\n",
    "  Args:\n",
    "    mcmc_sample: shape (num_train, ..., num_mcmc).\n",
    "    X: shape (num_train, feature_dim)\n",
    "    Y: shape (num_train, ...)\n",
    "  \n",
    "  Returns:\n",
    "    sample_processed: shape (num_train*num_mcmc, ...)\n",
    "    X_processed: shape (num_train*num_mcmc, feature_dim)\n",
    "    Y_processed: shape (num_train*num_mcmc, ...)\n",
    "  \"\"\"\n",
    "  num_train = mcmc_sample.shape.as_list()[0]\n",
    "  num_mcmc =  mcmc_sample.shape.as_list()[-1]\n",
    "  output_dims = mcmc_sample.shape.as_list()[1]\n",
    "  num_train_X = X.shape.as_list()[0]\n",
    "  num_train_Y = Y.shape.as_list()[0]\n",
    "  output_dims_Y = Y.shape.as_list()[-1]\n",
    "\n",
    "  assert num_train == num_train_X, f'num_train={num_train}, num_train_X={num_train_X}'\n",
    "  assert num_train == num_train_Y, f'num_train={num_train}, num_train_Y={num_train_Y}'\n",
    "  assert output_dims == output_dims_Y, f'output_dims={output_dims}, output_dims_Y={output_dims_Y}'\n",
    "\n",
    "  if debug_mode:\n",
    "    print(f'Process data with num_mcmc={num_mcmc}, num_train={num_train}, '\n",
    "          f'output_dims={output_dims}')\n",
    "  \n",
    "  # Process MCMC sample, row-major stacking (contiguous predictions per X).\n",
    "  mcmc_sample_ = tf.transpose(mcmc_sample, [0, 2, 1])  # (num_train, num_mcmc, ...).\n",
    "  mcmc_sample_ = tf.reshape(mcmc_sample_, [-1, output_dims])  # (num_train * num_mcmc, ...).\n",
    "\n",
    "  # Process X, row-major stacking from (num_train, orig_dims).\n",
    "  X_ = tf.repeat(X, num_mcmc, axis=0)  # (num_train * num_mcmc, feature_dims)\n",
    "  Y_ = tf.repeat(Y, num_mcmc, axis=0)  # (num_train * num_mcmc, output_dims)\n",
    "\n",
    "  return mcmc_sample_, X_, Y_\n",
    "\n",
    "\n",
    "def deflatten_mcmc_sample(mcmc_sample_new, X, unused_Y, debug_mode=False):\n",
    "  \"\"\"Reshapes new MCMC sample to (num_mcmc, num_train, ...).\n",
    "  \n",
    "  Given data with shape (num_train * num_mcmc, ...), the model \n",
    "  produces a sample of (num_mcmc_new, num_train*num_mcmc, ...). This function\n",
    "  averages over the num_mcmc_new dimension and returns a tensor with shape \n",
    "  (num_mcmc * num_mcmc_new, num_train, ...).\n",
    "\n",
    "  Args:\n",
    "    mcmc_sample_new: New MCMC samples of shape \n",
    "      (num_mcmc_new, num_train * num_mcmc, ...).\n",
    "    X: shape (num_train, feature_dim)\n",
    "    unused_Y: shape (num_train, ...)\n",
    "  \n",
    "  Returns:\n",
    "    Reshaped MCMC samples of shape (num_mcmc_new*num_mcmc, num_train, ...).\n",
    "  \"\"\"\n",
    "  del unused_Y\n",
    "  num_mcmc_new, num_total = mcmc_sample_new.shape.as_list()[:2]\n",
    "  num_train = X.shape.as_list()[0]\n",
    "  num_mcmc = int(num_total / num_train)\n",
    "\n",
    "  if debug_mode:\n",
    "    print(f'Process data with num_total={num_total}, num_train={num_train}, '\n",
    "          f'num_mcmc_new={num_mcmc_new}, num_mcmc={num_mcmc}')\n",
    "\n",
    "  # De-flatten back to original dimension, (num_mcmc_new, num_train, num_mcmc, ...).\n",
    "  mcmc_sample_ = tf.reshape(mcmc_sample_new, [num_mcmc_new, num_train, num_mcmc, -1])\n",
    "\n",
    "  # Change axis to original, (num_mcmc_new, num_mcmc, num_train, ...).\n",
    "  mcmc_sample_ = tf.transpose(mcmc_sample_, [0, 2, 1, 3])\n",
    "\n",
    "  # Collapse over MCMC dimension.\n",
    "  return tf.reshape(mcmc_sample_, [num_mcmc_new * num_mcmc, num_train, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CU9lHfYYqL9U"
   },
   "outputs": [],
   "source": [
    "# @title Model: prepare_mcmc\n",
    "\n",
    "def prepare_mcmc(\n",
    "    model_prior, y, param_name='gp_weights', nchain=10, seed=0):\n",
    "  init_state = model_prior.sample(nchain)[param_name]\n",
    "\n",
    "  def target_log_prob_fn(gp_w):\n",
    "    # conditional_sample = model_prior.sample(batch_size)\n",
    "    # Only perform inference on gp_w.\n",
    "    conditional_sample = dict()\n",
    "    conditional_sample[param_name] = gp_w\n",
    "    conditional_sample['y'] = y\n",
    "\n",
    "    log_probs = model_prior.log_prob(conditional_sample)\n",
    "    return tf.reduce_mean(log_probs)\n",
    "\n",
    "  return init_state, target_log_prob_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "624-vESuopFM"
   },
   "outputs": [],
   "source": [
    "# @title Model: make_bma_samples.\n",
    "def make_bma_samples(X, Y, base_model_preds, \n",
    "                     bma_weight_samples, \n",
    "                     bma_model_config, n_samples=50, seed=0, \n",
    "                     y_samples_only=True,\n",
    "                     prepare_mcmc_training=False,\n",
    "                     debug_mode=False):\n",
    "  # Defines posterior distribution.\n",
    "  bma_posterior, _ = bma_dist(X, base_model_preds,\n",
    "                              posterior_mode=True,\n",
    "                              posterior_sample=bma_weight_samples,\n",
    "                              sample_intermediate_variables=not y_samples_only,\n",
    "                              debug_mode=debug_mode,\n",
    "                              **bma_model_config)  \n",
    "\n",
    "  # Samples from posterior. Shape (num_mcmc, num_data, num_output).\n",
    "  bma_samples = bma_posterior.sample(n_samples, seed=seed)\n",
    "\n",
    "  if y_samples_only:\n",
    "    # Only returns samples for y, shape (num_mcmc, num_data, num_output).\n",
    "    bma_samples = bma_samples[\"y\"]\n",
    "\n",
    "    if prepare_mcmc_training:\n",
    "      # Reshapes to (num_data, num_output, num_mcmc).\n",
    "      bma_samples = tf.transpose(bma_samples, [1, 2, 0])\n",
    "\n",
    "      # Reshape to (num_data * num_mcmc, ...).\n",
    "      return process_mcmc_data(bma_samples, X, Y)\n",
    "\n",
    "  del bma_posterior\n",
    "  return bma_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AbE6ssMfyuCR"
   },
   "outputs": [],
   "source": [
    "# @title Model: make_bne_samples\n",
    "def make_bne_samples(X: tf.Tensor, \n",
    "                     mean_preds: tf.Tensor,\n",
    "                     bne_weight_samples: tf.Tensor, \n",
    "                     bne_model_config: Dict[str, Any], \n",
    "                     seed: int = 0, \n",
    "                     debug_mode: bool = False):\n",
    "  \"\"\"Generates posterior predictive samples for BNE.\n",
    "  \n",
    "  Args: \n",
    "    X: Input features, shape (num_data, num_input_dim).\n",
    "    mean_preds: Predictive samples from a basic ensemble model, \n",
    "      shape (num_pred_samples, num_data, 1).\n",
    "    bne_weight_samples: Posterior samples for random feature weights of the BNE\n",
    "      model, shape (num_hidden_units, num_param_dim, num_bne_samples). The \n",
    "      `num_bne_samples` here is the number of posterior sample for the BNE's \n",
    "      internal parameters and does not need to equal to `num_pred_samples` from\n",
    "       `mean_preds`.\n",
    "    bne_model_config: Keyword arguments to `bne_model_dist`.\n",
    "    seed: Random seed for posterior sampler.\n",
    "    debug_mode: Whether to print out intermediate debugging information.\n",
    "\n",
    "  Returns:\n",
    "    bne_samples: A dictionary of BNE posterior predictions, with fields:\n",
    "     * 'bne_gp_w': random feature weights, \n",
    "        (num_pred_samples, num_hidden_units, num_params)\n",
    "     * 'bne_gp': GP posteriors evaluated at X, shape \n",
    "        (num_pred_samples, num_params)\n",
    "     * 'mean': the mean parameter for the residual process,\n",
    "       (num_pred_samples, 1)\n",
    "     * 'scale': the scale (variance) parameter \n",
    "        (num_pred_samples, 1)\n",
    "     * 'rate': the rate parameter for modeling skewness\n",
    "        (num_pred_samples, 1). \n",
    "        Available only when `estimate_skewness=True` in bne_model_dist.\n",
    "     * 'skewness': the skewness parameter computed as \n",
    "        tfd.Exponential(rate) (num_pred_samples, 1). Available only when \n",
    "        `estimate_skewness=True` in bne_model_dist.\n",
    "     * 'mean_original': The predictive distribution from the original ensemble\n",
    "        model, (num_pred_samples, 1).\n",
    "     * 'y': The predictive distribution of the final model ensmeble, \n",
    "        (num_pred_samples, 1).\n",
    "  \"\"\"\n",
    "  # Identify the number of predictive samples to create.\n",
    "  num_pred_samples = mean_preds.shape[0]\n",
    "\n",
    "  if debug_mode:\n",
    "    print(\n",
    "        f'Generate {num_pred_samples} predictive samples.'\n",
    "        f'based on mean_preds with shape {mean_preds.shape}')\n",
    "  \n",
    "  # Construct the residual posterior. \n",
    "  bne_posterior, _ = bne_model_dist(\n",
    "      inputs=X,\n",
    "      mean_preds=0.,  # Only generate residual sample by fixing mean at zero.\n",
    "      posterior_mode=True,\n",
    "      posterior_sample=bne_weight_samples,\n",
    "      sample_intermediate_variables=True,\n",
    "      debug_mode=debug_mode,\n",
    "      **bne_model_config)\n",
    "\n",
    "  if debug_mode:\n",
    "    print(f'BNE Posterior Model Graph: {bne_posterior.resolve_graph()}')\n",
    "\n",
    "  # Sample from residual posterior.\n",
    "  bne_samples = bne_posterior.sample(num_pred_samples, seed=seed)\n",
    "  \n",
    "  # Construct full model prediction by adding mean. Shape (num_sample, num_data, 1)\n",
    "  bne_samples['mean_original'] = mean_preds\n",
    "  bne_samples['resid'] = bne_samples['y']  \n",
    "  bne_samples['y'] = bne_samples['resid'] + mean_preds  \n",
    "\n",
    "  del bne_posterior  \n",
    "  return bne_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHS-GXDOJcRd"
   },
   "source": [
    "## Simulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "SWDlFDVSJf9d"
   },
   "outputs": [],
   "source": [
    "# @title Simulation: get_data\n",
    "def get_data(seed, N_train, y_dist, data_gen_fn, num_train_steps=100):\n",
    "  \"\"\"Generates data and trains base models.\"\"\"\n",
    "  (X_base, X_train, X_test, \n",
    "   Y_base, Y_train, Y_test, \n",
    "   mean_test) = data_gen_fn(N_train, N_base, N_test, \n",
    "                            y_dist=y_dist, seed=seed)\n",
    "\n",
    "  base_preds_train, base_preds_test, kernel_names = run_base_models(\n",
    "      X_base, X_train, X_test, Y_base, Y_train, Y_test, \n",
    "      num_train_steps=num_train_steps)\n",
    "  \n",
    "  data_dict = dict(X_base=X_base,\n",
    "                   X_train=X_train,\n",
    "                   X_test=X_test,\n",
    "                   Y_base=Y_base, \n",
    "                   Y_train=Y_train, \n",
    "                   Y_test=Y_test, \n",
    "                   mean_test=mean_test, \n",
    "                   base_preds_train=base_preds_train, \n",
    "                   base_preds_test=base_preds_test, \n",
    "                   base_model_names=kernel_names)\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ikgl1P7oJhLV"
   },
   "outputs": [],
   "source": [
    "# @title Simulation: get_bma_result\n",
    "def get_bma_result(data_dict, bma_config):\n",
    "  \"\"\"Trains Adaptive Bayesian model averaging.\"\"\"\n",
    "  (bma_joint_samples, X_train_mcmc, Y_train_mcmc, \n",
    "   means_train_mcmc, means_test_mcmc) = run_bma_model(\n",
    "       X_train=data_dict[\"X_train\"], \n",
    "       X_test=data_dict[\"X_test\"], \n",
    "       Y_train=data_dict[\"Y_train\"], \n",
    "       base_preds_train=data_dict[\"base_preds_train\"], \n",
    "       base_preds_test=data_dict[\"base_preds_test\"], \n",
    "       return_mcmc_examples=True,\n",
    "       **bma_config)\n",
    "\n",
    "  data_dict['X_train_mcmc'] = X_train_mcmc\n",
    "  data_dict['Y_train_mcmc'] = Y_train_mcmc\n",
    "  data_dict['means_train_mcmc'] = means_train_mcmc\n",
    "  data_dict['means_test_mcmc'] = means_test_mcmc\n",
    "  data_dict['bma_mean_samples'] = bma_joint_samples['y']\n",
    "\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mQ3T1JVdJhT2"
   },
   "outputs": [],
   "source": [
    "# @title Simulation: get_bne_result\n",
    "def get_bne_result(data_dict, moment_mode, bne_config):\n",
    "  \"\"\"Trains Bayesian nonparametric ensemble.\"\"\"\n",
    "  mode_to_name_map = {'none': 'bma', 'mean': 'bae', \n",
    "                      'variance': 'bne_var', 'skewness': 'bne_skew'}\n",
    "  model_name = mode_to_name_map[moment_mode]\n",
    "\n",
    "  joint_samples = run_bne_model(X_train=data_dict['X_train_mcmc'], \n",
    "                                Y_train=data_dict['Y_train_mcmc'], \n",
    "                                X_test=data_dict['X_test'],\n",
    "                                base_model_samples_train=data_dict['means_train_mcmc'],\n",
    "                                base_model_samples_test=data_dict['means_test_mcmc'],\n",
    "                                moment_mode=moment_mode,\n",
    "                                **bne_config)\n",
    "  \n",
    "  data_dict[f'{model_name}_samples'] = joint_samples['y']\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "w7v_gxoYJhXP"
   },
   "outputs": [],
   "source": [
    "# # @title Simulation: save/load_from_drive\n",
    "\n",
    "# def save_to_drive(data, file_name, file_path):\n",
    "#   path_name = os.path.join(file_path, f'{file_name}.pkl')\n",
    "#   with tf.io.gfile.GFile(path_name, 'wb') as f:\n",
    "#     pickle.dump(data, f)\n",
    "\n",
    "# def load_from_drive(file_name, file_path):  \n",
    "#   path_name = os.path.join(file_path, f'{file_name}.pkl')\n",
    "#   with tf.io.gfile.GFile(path_name, 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "#   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "N-18GdxrKzbI"
   },
   "outputs": [],
   "source": [
    "# @title Simulation: compute_metrics\n",
    "def compute_metrics(data_dict, model_name, q_true=None, ind_ids=None, num_sample=None):\n",
    "  if q_true is None:\n",
    "    q_true = np.array(\n",
    "        [0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25,\n",
    "         0.75, 0.8, 0.85, 0.9, 0.925, 0.95, 0.975])\n",
    "\n",
    "  if ind_ids is None:\n",
    "    # Find IDs of in-domain test data via range comparison \n",
    "    # between X_train and X_test.\n",
    "    X_train_min = np.min(data_dict['X_train'], axis=0)\n",
    "    X_train_max = np.max(data_dict['X_train'], axis=0)\n",
    "\n",
    "    test_ids_greater_than_min = np.all(\n",
    "        data_dict['X_test'] > X_train_min, axis=-1)\n",
    "    test_ids_less_than_max = np.all(\n",
    "        data_dict['X_test'] < X_train_max, axis=-1)\n",
    "\n",
    "    ind_ids = np.where(\n",
    "        np.logical_and(test_ids_greater_than_min, test_ids_less_than_max))[0]\n",
    "\n",
    "  samples = data_dict[f'{model_name}_samples']\n",
    "  means_true = data_dict['mean_test']\n",
    "  y_test = data_dict['Y_test']\n",
    "\n",
    "  if num_sample is not None:\n",
    "    samples = samples[:num_sample]\n",
    "\n",
    "  means_pred = np.mean(samples, axis=0)\n",
    "  stds_pred = np.std(samples, axis=0)\n",
    "  quantile_pred = np.quantile(samples, q=q_true, axis=0)\n",
    "\n",
    "  # Compute in-domain metrics.\n",
    "  nll_ind = np.mean(\n",
    "      ((means_pred[ind_ids] - means_true[ind_ids])/stds_pred[ind_ids])**2 + \n",
    "      np.log(stds_pred[ind_ids]))\n",
    "  clb_ind = np.mean(\n",
    "      ((means_pred[ind_ids] - means_true[ind_ids])/stds_pred[ind_ids])**2)\n",
    "  shp_ind = np.mean(np.log(stds_pred[ind_ids]))\n",
    "  mse_ind = np.mean(\n",
    "      (means_pred[ind_ids] - means_true[ind_ids])**2) / np.var(means_true[ind_ids])\n",
    "\n",
    "  q_pred_ind = np.mean(y_test[ind_ids] < quantile_pred[:, ind_ids], axis=(1, 2))\n",
    "  ece_ind = np.mean((q_pred_ind - q_true)**2)\n",
    "  cov_prob_95_ind = q_pred_ind[-1] - q_pred_ind[0]\n",
    "  cov_prob_90_ind = q_pred_ind[-2] - q_pred_ind[1]\n",
    "  cov_prob_85_ind = q_pred_ind[-3] - q_pred_ind[2]\n",
    "  cov_prob_80_ind = q_pred_ind[-4] - q_pred_ind[3]\n",
    "\n",
    "  # Compute all-domain (ind + ood) metrics.\n",
    "  nll_all = np.mean(((means_pred - means_true)/stds_pred)**2 + np.log(stds_pred))\n",
    "  clb_all = np.mean(((means_pred - means_true)/stds_pred)**2)\n",
    "  shp_all = np.mean(np.log(stds_pred))\n",
    "  mse_all = np.mean((means_pred - means_true)**2) / np.var(means_true)\n",
    "\n",
    "  q_pred_all = np.mean(y_test < quantile_pred, axis=(1, 2))\n",
    "  ece_all = np.mean((q_pred_all - q_true)**2)\n",
    "  cov_prob_95_all = q_pred_all[-1] - q_pred_all[0]\n",
    "  cov_prob_90_all = q_pred_all[-2] - q_pred_all[1]\n",
    "  cov_prob_85_all = q_pred_all[-3] - q_pred_all[2]\n",
    "  cov_prob_80_all = q_pred_all[-4] - q_pred_all[3]\n",
    "\n",
    "  return (mse_ind, nll_ind, clb_ind, shp_ind, ece_ind, cov_prob_95_ind, cov_prob_90_ind, cov_prob_85_ind, cov_prob_80_ind,\n",
    "          mse_all, nll_all, clb_all, shp_all, ece_all, cov_prob_95_all, cov_prob_90_all, cov_prob_85_all, cov_prob_80_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TI0wSe45Jhdy"
   },
   "outputs": [],
   "source": [
    "# @title Simulation: run_pipeline\n",
    "def run_pipeline(seeds, N_train, group_id, data_gen_fn, base_train_steps=200):\n",
    "  # Data Generation\n",
    "  data_dicts = {}\n",
    "  print('Data:', end='', flush=True)\n",
    "  t0 = time.time()\n",
    "  for seed in seeds:\n",
    "    print(f'Run {seed+1}...', end='', flush=True)\n",
    "    data_dicts[seed] = get_data(seed, N_train, y_dist, data_gen_fn, \n",
    "                                num_train_steps=base_train_steps)\n",
    "  print(f'Time: {(time.time()-t0)/60.:.4f} min.')\n",
    "\n",
    "  # BMA-mean.\n",
    "  print('BMA-mean:', flush=True)\n",
    "  t0 = time.time()\n",
    "  for seed in seeds:\n",
    "    print(f'Run {seed+1}: ', end='', flush=True)\n",
    "    data_dicts[seed] = get_bma_result(data_dicts[seed], bma_config=bma_config) \n",
    "  print(f'Time: {(time.time()-t0)/60.:.4f} min.', flush=True)\n",
    "  tf.keras.backend.clear_session()\n",
    "  gc.collect()\n",
    "\n",
    "  # BMA.\n",
    "  print('BMA:', flush=True)\n",
    "  # Inhere BMA MCMC configs.\n",
    "  bma_var_config = bne_config.copy()\n",
    "  bma_var_config['mcmc_initialize_from_map'] = bma_config['mcmc_initialize_from_map']\n",
    "  t0 = time.time()\n",
    "  for seed in seeds:\n",
    "    print(f'Run {seed+1}: ', end='', flush=True)\n",
    "    data_dicts[seed] = get_bne_result(data_dicts[seed], moment_mode='none', \n",
    "                                      bne_config=bma_var_config) \n",
    "  print(f'Time: {(time.time()-t0)/60.:.4f} min.', flush=True)\n",
    "  tf.keras.backend.clear_session()\n",
    "  gc.collect()\n",
    "\n",
    "  # BAE.\n",
    "  print('BAE:', flush=True)\n",
    "  t0 = time.time()\n",
    "  for seed in seeds:\n",
    "    print(f'Run {seed+1}: ', end='', flush=True)\n",
    "    data_dicts[seed] = get_bne_result(data_dicts[seed], moment_mode='mean', \n",
    "                                      bne_config=bne_config) \n",
    "  print(f'Time: {(time.time()-t0)/60.:.4f} min.', flush=True)\n",
    "  tf.keras.backend.clear_session()\n",
    "  gc.collect()\n",
    "\n",
    "  # BNE-Variance.\n",
    "  print('BNE-Variance:', flush=True)\n",
    "  t0 = time.time()\n",
    "  for seed in seeds:\n",
    "    print(f'Run {seed+1}: ', end='', flush=True)\n",
    "    data_dicts[seed] = get_bne_result(data_dicts[seed], moment_mode='variance', \n",
    "                                      bne_config=bne_config) \n",
    "  print(f'Time: {(time.time()-t0)/60.:.4f} min.', flush=True)\n",
    "  tf.keras.backend.clear_session()\n",
    "  gc.collect()\n",
    "\n",
    "  # BNE-Skewness.\n",
    "  print('BNE-Skewness:', flush=True)\n",
    "  t0 = time.time()\n",
    "  for seed in seeds:\n",
    "    print(f'Run {seed+1}: ', end='', flush=True)\n",
    "    data_dicts[seed] = get_bne_result(data_dicts[seed], moment_mode='skewness', \n",
    "                                      bne_config=bne_config) \n",
    "  print(f'Time: {(time.time()-t0)/60.:.4f} min.', flush=True)\n",
    "  tf.keras.backend.clear_session()\n",
    "  gc.collect()\n",
    "\n",
    "  return data_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Nmbq-0wgYy"
   },
   "source": [
    "# Default Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization configs. \n",
    "# Consider reduce below parameters / set to `False` if MCMC is taking too long:\n",
    "# mcmc_num_steps, mcmc_burnin, mcmc_nchain, mcmc_initialize_from_map.\n",
    "map_step_size=5e-4   # @param\n",
    "map_num_steps=10_000  # @param\n",
    "\n",
    "mcmc_step_size=1e-4 # @param\n",
    "mcmc_num_steps=1000 # @param\n",
    "\n",
    "mcmc_nchain=1 # @param\n",
    "mcmc_burnin=100 # @param\n",
    "bne_mcmc_initialize_from_map=\"True\" # @param [\"False\", \"True\"]\n",
    "\n",
    "bne_mcmc_initialize_from_map = eval(bne_mcmc_initialize_from_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMA parameters.\n",
    "y_noise_std = 0.01  # Note: Changed from 0.1 # @param\n",
    "bma_gp_lengthscale = 1. # @param\n",
    "bma_gp_l2_regularizer = 0.1 # @param\n",
    "\n",
    "bma_n_samples_train = 100 # @param\n",
    "bma_n_samples_eval = 250 # @param\n",
    "bma_n_samples_test = 250 # @param\n",
    "bma_seed = 0 # @param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE parameters.\n",
    "bne_gp_lengthscale = 4 # 5. # @param\n",
    "bne_gp_l2_regularizer = 5 # 15 # @param\n",
    "bne_variance_prior_mean = -2.5 # @param\n",
    "bne_skewness_prior_mean = -2.5 # @param\n",
    "bne_seed = 0 # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bma_config=dict(gp_lengthscale=bma_gp_lengthscale,\n",
    "                gp_l2_regularizer=bma_gp_l2_regularizer,\n",
    "                y_noise_std=y_noise_std,\n",
    "                map_step_size=map_step_size,\n",
    "                map_num_steps=map_num_steps,\n",
    "                mcmc_step_size=mcmc_step_size,\n",
    "                mcmc_num_steps=mcmc_num_steps,\n",
    "                mcmc_initialize_from_map=False,\n",
    "                n_samples_eval=bma_n_samples_eval,\n",
    "                n_samples_train=bma_n_samples_train,\n",
    "                n_samples_test=bma_n_samples_test,\n",
    "                seed=bma_seed)\n",
    "\n",
    "bne_config = dict(gp_lengthscale=bne_gp_lengthscale,\n",
    "                  gp_l2_regularizer=bne_gp_l2_regularizer,\n",
    "                  variance_prior_mean=bne_variance_prior_mean,\n",
    "                  skewness_prior_mean=bne_skewness_prior_mean,\n",
    "                  map_step_size=map_step_size,\n",
    "                  map_num_steps=map_num_steps,\n",
    "                  mcmc_step_size=mcmc_step_size,\n",
    "                  mcmc_num_steps=mcmc_num_steps,\n",
    "                  mcmc_nchain=mcmc_nchain,\n",
    "                  mcmc_burnin=mcmc_burnin,\n",
    "                  mcmc_initialize_from_map=bne_mcmc_initialize_from_map,\n",
    "                  seed=bne_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "zCuDvZJ71_Dz"
   },
   "outputs": [],
   "source": [
    "# GP configs.\n",
    "y_noise_std = 0.1  # @param\n",
    "hidden_units = 128  # @param\n",
    "lengthscale=1.  # @param\n",
    "l2_regularizer=0.1  # @param\n",
    "\n",
    "DEFAULT_GP_CONFIG = dict(lengthscale=lengthscale,\n",
    "                         l2_regularizer=l2_regularizer, \n",
    "                         hidden_units=hidden_units, \n",
    "                         y_noise_std=y_noise_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fY1Z6ccs2XUP"
   },
   "outputs": [],
   "source": [
    "# BNE model configs.\n",
    "estimate_mean = \"True\" # @param [\"True\", \"False\"]\n",
    "estimate_variance = \"False\" # @param [\"True\", \"False\"]\n",
    "estimate_skewness = \"False\" # @param [\"True\", \"False\"]\n",
    "variance_prior_mean=0. # @param\n",
    "skewness_prior_mean=0. # @param\n",
    "\n",
    "estimate_mean = eval(estimate_mean)\n",
    "estimate_variance = eval(estimate_variance)\n",
    "estimate_skewness = eval(estimate_skewness)\n",
    "\n",
    "DEFAULT_BNE_CONFIG = dict(estimate_mean=estimate_mean,\n",
    "                          estimate_variance=estimate_variance,\n",
    "                          estimate_skewness=estimate_skewness,\n",
    "                          variance_prior_mean=variance_prior_mean,\n",
    "                          skewness_prior_mean=skewness_prior_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "L_JG6Hqn13o7"
   },
   "outputs": [],
   "source": [
    "# MAP configs.\n",
    "map_step_size=0.1 # @param\n",
    "map_num_steps=10_000 # @param\n",
    "\n",
    "DEFAULT_MAP_CONFIG = dict(learning_rate=map_step_size,\n",
    "                          num_steps=map_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "R0cNR_kHwmdS"
   },
   "outputs": [],
   "source": [
    "# MCMC configs.\n",
    "mcmc_step_size=0.1 # @param\n",
    "mcmc_sample_size=500 # @param\n",
    "mcmc_num_steps=10_000 # @param\n",
    "mcmc_burnin=2_500 # @param\n",
    "mcmc_nchain=10 # @param\n",
    "mcmc_seed=0 # @param\n",
    "\n",
    "DEFAULT_MCMC_CONFIG = dict(step_size=mcmc_step_size, \n",
    "                           num_steps=mcmc_sample_size, \n",
    "                           burnin=mcmc_burnin, \n",
    "                           nchain=mcmc_nchain, \n",
    "                           seed=mcmc_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training/prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_estimates_USA = pd.read_csv('../../data/merged_fb_census_data_280922.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.asarray(pop_estimates_USA[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "\n",
    "#train_ind = range(1,100,2)\n",
    "#test_ind = range(2,100,2)\n",
    "train_ind = range(1,pop_estimates_USA.shape[0],2)\n",
    "test_ind = range(2,pop_estimates_USA.shape[0],2)\n",
    "\n",
    "# standardize\n",
    "X_train1 = X_train1[train_ind,]\n",
    "X_centr = np.mean(X_train1, axis=0)\n",
    "X_scale = np.max(X_train1, axis=0) - np.min(X_train1, axis=0)\n",
    "\n",
    "X_train1 = (X_train1 - X_centr) / X_scale\n",
    "\n",
    "X_test1 = np.asarray(pop_estimates_USA[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "\n",
    "# standardize\n",
    "X_test1 = X_test1[test_ind,]\n",
    "X_centr = np.mean(X_test1, axis=0)\n",
    "X_scale = np.max(X_test1, axis=0) - np.min(X_test1, axis=0)\n",
    "\n",
    "X_test1 = (X_test1 - X_centr) / X_scale\n",
    "Y = np.expand_dims(pop_estimates_USA[\"census\"], 1)\n",
    "Y_train = Y[train_ind]\n",
    "Y_test = Y[test_ind]\n",
    "\n",
    "base_models = np.asarray(pop_estimates_USA[[\"acs\",\"pep\",\"worldpop\",\"fb\"]])\n",
    "base_models = np.asarray(base_models)\n",
    "base_train = base_models[train_ind,]\n",
    "base_test = base_models[test_ind,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where does the model come in? If X just determines the location and thus will be used to get the distances between locations, where do the actual input models come in?\n",
    "#base_preds_train, base_preds_test, kernel_names = run_base_models(\n",
    "    #  X_train1, X_train1, X_test1, Y_train, Y_train, Y_test, num_train_steps=100, debug_mode=False)\n",
    "## We want to re-write this to just use our data sources. Duh.\n",
    "\n",
    "d0 = dict(X_base=X_train1,\n",
    "                   X_train=X_train1,\n",
    "                   X_test=X_test1,\n",
    "                   Y_base=Y_train, \n",
    "                   Y_train=Y_train, \n",
    "                   Y_test=Y_test, \n",
    "                   mean_test=Y_test, \n",
    "                   base_preds_train=base_train, \n",
    "                   base_preds_test=base_test, \n",
    "                   base_model_names=[\"acs\",\"pep\",\"worldpop\",\"fb\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MCMC:\tAcceptance Ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    " data_dicts = get_bma_result(d0, bma_config=bma_config) \n",
    "    ## ENDED HERE AS OF 6pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MCMC:\tAcceptance Ratio: 0.0\n",
      "Running MCMC:\tAcceptance Ratio: 0.0\n",
      "Running MAP:\tnan...nan...nan...nan...nan...nan...nan...nan...nan...nan...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.0\n",
      "dict_keys(['X_base', 'X_train', 'X_test', 'Y_base', 'Y_train', 'Y_test', 'mean_test', 'base_preds_train', 'base_preds_test', 'base_model_names', 'X_train_mcmc', 'Y_train_mcmc', 'means_train_mcmc', 'means_test_mcmc', 'bma_mean_samples', 'bma_samples', 'bae_samples'])\n",
      "Running MAP:\tnan...nan...nan...nan...nan...nan...nan...nan...nan...nan...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.0\n",
      "Running MAP:\tnan...nan...nan...nan...nan...nan...nan...nan...nan...nan...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "\n",
    "def run_single2D():\n",
    "    # Data Generation\n",
    "    #data_dicts = {}\n",
    "    data_dicts = d0\n",
    "    # dict_keys(['X_base', 'X_train', 'X_test', 'Y_base', 'Y_train', 'Y_test', 'mean_test', 'base_preds_train', 'base_preds_test', 'base_model_names'])\n",
    "\n",
    "    # BMA-mean.\n",
    "    data_dicts = get_bma_result(d0, bma_config=bma_config) \n",
    "    \n",
    "    # BMA.\n",
    "    # Inhere BMA MCMC configs.\n",
    "    bma_var_config = bne_config.copy()\n",
    "    bma_var_config['mcmc_initialize_from_map'] = bma_config['mcmc_initialize_from_map']\n",
    "    data_dicts = get_bne_result(data_dicts, moment_mode='none', \n",
    "                                      bne_config=bma_var_config) \n",
    "\n",
    "    # BAE.\n",
    "    data_dicts = get_bne_result(data_dicts, moment_mode='mean', \n",
    "                                      bne_config=bne_config) \n",
    "    print(data_dicts.keys())\n",
    "\n",
    "    # BNE-Variance.\n",
    "    data_dicts = get_bne_result(data_dicts, moment_mode='variance', \n",
    "                                      bne_config=bne_config) \n",
    "\n",
    "\n",
    "    # BNE-Skewness.\n",
    "    data_dicts = get_bne_result(data_dicts, moment_mode='skewness', \n",
    "                                      bne_config=bne_config) \n",
    "\n",
    "    return data_dicts\n",
    "\n",
    "plt_dict1 = run_single2D()    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  27457],\n",
       "       [  57322],\n",
       "       [  20947],\n",
       "       [  34215],\n",
       "       [  43643],\n",
       "       [  25833],\n",
       "       [  14972],\n",
       "       [  54428],\n",
       "       [  11539],\n",
       "       [  13906],\n",
       "       [  50251],\n",
       "       [  71109],\n",
       "       [  38319],\n",
       "       [  17241],\n",
       "       [  26790],\n",
       "       [  15760],\n",
       "       [ 101547],\n",
       "       [ 658466],\n",
       "       [  92709],\n",
       "       [ 140247],\n",
       "       [  11299],\n",
       "       [ 334811],\n",
       "       [  30776],\n",
       "       [ 412992],\n",
       "       [ 229363],\n",
       "       [  10591],\n",
       "       [  32899],\n",
       "       [  52947],\n",
       "       [ 195085],\n",
       "       [  82291],\n",
       "       [ 194656],\n",
       "       [  17581],\n",
       "       [  24484],\n",
       "       [ 131346],\n",
       "       [  53597],\n",
       "       [   8437],\n",
       "       [3817117],\n",
       "       [ 107449],\n",
       "       [ 375770],\n",
       "       [ 211033],\n",
       "       [  19019],\n",
       "       [  41513],\n",
       "       [  36903],\n",
       "       [   5368],\n",
       "       [  11800],\n",
       "       [  16083],\n",
       "       [   8689],\n",
       "       [  21273],\n",
       "       [  61948]], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt_dict1['Y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_base', 'X_train', 'X_test', 'Y_base', 'Y_train', 'Y_test', 'mean_test', 'base_preds_train', 'base_preds_test', 'base_model_names', 'X_train_mcmc', 'Y_train_mcmc', 'means_train_mcmc', 'means_test_mcmc', 'bma_mean_samples', 'bma_samples', 'bae_samples', 'bne_var_samples', 'bne_skew_samples'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5000, 2), dtype=float32, numpy=\n",
       "array([[ 0.14189821, -0.48514947],\n",
       "       [ 0.14189821, -0.48514947],\n",
       "       [ 0.14189821, -0.48514947],\n",
       "       ...,\n",
       "       [ 0.05226918,  0.31135854],\n",
       "       [ 0.05226918,  0.31135854],\n",
       "       [ 0.05226918,  0.31135854]], dtype=float32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(plt_dict1.keys())\n",
    "plt_dict1['X_train_mcmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eastMA_pred = np.mean(plt_dict1[\"bne_var_samples\"], axis=0)\n",
    "plt.scatter(plt_dict1[\"X_test\"][:,0],plt_dict1[\"X_test\"][:,1], c=eastMA_pred, alpha=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [i for i in plt_dict25.keys()]\n",
    "# for i in a:\n",
    "#     print(i, len(plt_dict25[i]))\n",
    "# b = [i for i in plt_dict1000.keys()]\n",
    "# for i in b:\n",
    "#     print(i, len(plt_dict1000[i]))\n",
    "# data_dicts1 = get_data(1, 25, np.random.lognormal, generate_data_2d, \n",
    "#                                 num_train_steps=200)#base_train_steps\n",
    "# display(X_train.shape,X_train1.shape,Y_train.shape,training_eastMA[\"aqs\"].shape, b.shape) \n",
    "# display(plt_dict0[\"X_train\"].shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(plt_dict1.keys())\n",
    "# display(plt_dict1[\"bne_var_samples\"])\n",
    "# pd.DataFrame(plt_dict1[\"bne_var_samples\"][:,:,0])\n",
    "# plt_dict1[\"bne_var_samples\"][1,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper: Plot Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_heatmap_2d(plot_data, X,\n",
    "                         X_monitor=None,\n",
    "                         cmap='inferno_r',\n",
    "                         norm=None, norm_method=\"percentile\",\n",
    "                         save_addr=''):\n",
    "    \"\"\"Plots colored 2d heatmap using scatterplot.\n",
    "\n",
    "    Args:\n",
    "        plot_data: (np.ndarray) plot data whose color to visualize over\n",
    "            2D surface, shape (N, ).\n",
    "        X: (np.ndarray) locations of the plot data, shape (N, 2).\n",
    "        X_monitor: (np.ndarray or None) Locations to plot data points to.\n",
    "        cmap: (str) Name of color map.\n",
    "        norm: (BoundaryNorm or None) Norm values to adjust color map.\n",
    "            If None then a new norm will be created according to norm_method.\n",
    "        norm_method: (str) The name of method to compute norm values.\n",
    "            See util.visual.make_color_norm for detail.\n",
    "        save_addr: (str) Address to save image to.\n",
    "\n",
    "    Returns:\n",
    "        (matplotlib.colors.BoundaryNorm) A color norm object for color map\n",
    "            to be passed to a matplotlib.pyplot function.\n",
    "    \"\"\"\n",
    "#     if save_addr:\n",
    "#         pathlib.Path(save_addr).parent.mkdir(parents=True, exist_ok=True)\n",
    "#         plt.ioff()\n",
    "\n",
    "#     if not norm:\n",
    "#         norm = make_color_norm(plot_data, method=norm_method)\n",
    "\n",
    "    # 2d color plot using scatter\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(x=X[:, 0], y=X[:, 1],\n",
    "                s=3,\n",
    "                c=plot_data, cmap=cmap, norm=norm)\n",
    "    cbar = plt.colorbar()\n",
    "\n",
    "    #     plot monitors\n",
    "    if isinstance(X_monitor, np.ndarray):\n",
    "        plt.scatter(x=X_monitor[:, 0], y=X_monitor[:, 1],\n",
    "                    s=10, c='black')\n",
    "\n",
    "    # adjust plot window\n",
    "    plt.xlim((np.min(X[:, 0]), np.max(X[:, 0])))\n",
    "    plt.ylim((np.min(X[:, 1]), np.max(X[:, 1])))\n",
    "\n",
    "    if save_addr:\n",
    "        plt.savefig(save_addr, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.ion()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "posterior_heatmap_2d(eastMA_pred, plt_dict1[\"X_test\"],\n",
    "                         X_train1,\n",
    "                         cmap='RdYlGn_r',\n",
    "                         norm=None, norm_method=\"percentile\",\n",
    "                         save_addr='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "eastMA_pred_var = np.var(plt_dict1[\"bne_var_samples\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "posterior_heatmap_2d(eastMA_pred_var, plt_dict1[\"X_test\"],\n",
    "                         X_train1,\n",
    "                         cmap='inferno_r',\n",
    "                         norm=None, norm_method=\"percentile\",\n",
    "                         save_addr='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWSzplhQKdqt"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "2LLseEkxKIqA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_drive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m data_dicts \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seed_groups)):\n\u001b[1;32m---> 10\u001b[0m   data_dict_group \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_drive\u001b[49m(\n\u001b[0;32m     11\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, file_path\u001b[38;5;241m=\u001b[39mFULL_DATA_PATH)\n\u001b[0;32m     12\u001b[0m   data_dicts\u001b[38;5;241m.\u001b[39mupdate(data_dict_group)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbae\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbne_var\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbne_skew\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_from_drive' is not defined"
     ]
    }
   ],
   "source": [
    "### Doesn't currently work for this data setup\n",
    "# Compute metrics for all N_train and models. \n",
    "metric_rows = []\n",
    "\n",
    "N_train_grid = [50] \n",
    "seed_groups = [1]\n",
    "\n",
    "for N_train in N_train_grid:\n",
    "  data_dicts = {}\n",
    "  for group_id in range(len(seed_groups)):\n",
    "    data_dict_group = load_from_drive(\n",
    "        f'result_{N_train}_{group_id}', file_path=FULL_DATA_PATH)\n",
    "    data_dicts.update(data_dict_group)\n",
    "\n",
    "  for model_name in ('bma', 'bae', 'bne_var', 'bne_skew'):\n",
    "    metrics = [compute_metrics(data, model_name, num_sample=50) \n",
    "               for data in data_dicts.values()]\n",
    "    metrics = np.stack(metrics)\n",
    "    metric_means = np.mean(metrics, axis=0)\n",
    "    metric_stds = np.std(metrics, axis=0)\n",
    "\n",
    "    metric_row = dict(n=N_train, \n",
    "                      model_name=model_name, \n",
    "                      # Metric means.\n",
    "                      mse_ind=metric_means[0], \n",
    "                      nll_ind=metric_means[1], \n",
    "                      clb_ind=metric_means[2], \n",
    "                      shp_ind=metric_means[3], \n",
    "                      ece_ind=metric_means[4], \n",
    "                      cov_prob_95_ind=metric_means[5],\n",
    "                      mse_all=metric_means[9], \n",
    "                      nll_all=metric_means[10], \n",
    "                      clb_all=metric_means[11], \n",
    "                      shp_all=metric_means[12], \n",
    "                      ece_all=metric_means[13], \n",
    "                      cov_prob_95_all=metric_means[14],\n",
    "                      # Metric STDs.\n",
    "                      mse_ind_std=metric_stds[0], \n",
    "                      nll_ind_std=metric_stds[1], \n",
    "                      clb_ind_std=metric_stds[2], \n",
    "                      shp_ind_std=metric_stds[3], \n",
    "                      ece_ind_std=metric_stds[4], \n",
    "                      cov_prob_95_ind_std=metric_stds[5],\n",
    "                      mse_all_std=metric_stds[9], \n",
    "                      nll_all_std=metric_stds[10], \n",
    "                      clb_all_std=metric_stds[11], \n",
    "                      shp_all_std=metric_stds[12], \n",
    "                      ece_all_std=metric_stds[13], \n",
    "                      cov_prob_95_all_std=metric_stds[14],\n",
    "                      )\n",
    "    \n",
    "    metric_rows.append(metric_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oFdbfztkKgTW",
    "outputId": "219a2280-7db6-49b1-ecc0-bb1fdecfa785"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_pd = pd.DataFrame(metric_rows)\n",
    "metric_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_preds_train, base_preds_test, kernel_names = run_base_models(\n",
    "#       plt_dict0['X_base'], plt_dict0['X_train'], plt_dict0['X_test'], plt_dict0['Y_base'], plt_dict0['Y_train'], \n",
    "#       plt_dict0['Y_test'], num_train_steps=100)\n",
    "# d0 = dict(X_base=plt_dict0['X_base'],\n",
    "#                    X_train=plt_dict0['X_train'],\n",
    "#                    X_test=plt_dict0['X_test'],\n",
    "#                    Y_base=plt_dict0['Y_base'], \n",
    "#                    Y_train=plt_dict0['Y_train'], \n",
    "#                    Y_test=plt_dict0['Y_test'], \n",
    "#                    mean_test=plt_dict0['mean_test'], \n",
    "#                    base_preds_train=base_preds_train, \n",
    "#                    base_preds_test=base_preds_test, \n",
    "#                    base_model_names=kernel_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(0, 5),\n",
       " range(5, 10),\n",
       " range(10, 15),\n",
       " range(15, 20),\n",
       " range(20, 25),\n",
       " range(25, 30),\n",
       " range(30, 35),\n",
       " range(35, 40),\n",
       " range(40, 45),\n",
       " range(45, 50)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "VaG6bpWaLMfl",
    "outputId": "1cc81a4a-bf96-4a4e-c7c1-09423c9246c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5ff86ede-de8f-4b21-ade5-6b84b6030157\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"7\" halign=\"left\">nll_ind</th>\n",
       "      <th colspan=\"3\" halign=\"left\">nll_all</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">cov_prob_95_ind</th>\n",
       "      <th colspan=\"7\" halign=\"left\">cov_prob_95_all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <th>25</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>25</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>25</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bae</th>\n",
       "      <td>-2.9085</td>\n",
       "      <td>-2.9041</td>\n",
       "      <td>-2.9014</td>\n",
       "      <td>-2.9369</td>\n",
       "      <td>-2.9919</td>\n",
       "      <td>-3.0142</td>\n",
       "      <td>-3.0332</td>\n",
       "      <td>-2.4143</td>\n",
       "      <td>-2.4009</td>\n",
       "      <td>-2.3901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9602</td>\n",
       "      <td>0.9635</td>\n",
       "      <td>0.9593</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>0.8788</td>\n",
       "      <td>0.8846</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.8998</td>\n",
       "      <td>0.8954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bma</th>\n",
       "      <td>-2.7185</td>\n",
       "      <td>-2.7150</td>\n",
       "      <td>-2.7012</td>\n",
       "      <td>-2.7662</td>\n",
       "      <td>-2.8025</td>\n",
       "      <td>-2.8372</td>\n",
       "      <td>-2.8615</td>\n",
       "      <td>-2.2770</td>\n",
       "      <td>-2.2524</td>\n",
       "      <td>-2.2542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9491</td>\n",
       "      <td>0.9508</td>\n",
       "      <td>0.9489</td>\n",
       "      <td>0.8712</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.8816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bne_skew</th>\n",
       "      <td>-2.9609</td>\n",
       "      <td>-2.9492</td>\n",
       "      <td>-2.9585</td>\n",
       "      <td>-3.0208</td>\n",
       "      <td>-3.0611</td>\n",
       "      <td>-3.0660</td>\n",
       "      <td>-3.1046</td>\n",
       "      <td>-2.4606</td>\n",
       "      <td>-2.4597</td>\n",
       "      <td>-2.4620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>0.9356</td>\n",
       "      <td>0.9372</td>\n",
       "      <td>0.9438</td>\n",
       "      <td>0.9440</td>\n",
       "      <td>0.9408</td>\n",
       "      <td>0.9422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bne_var</th>\n",
       "      <td>-2.9473</td>\n",
       "      <td>-2.9420</td>\n",
       "      <td>-2.9547</td>\n",
       "      <td>-3.0024</td>\n",
       "      <td>-3.0403</td>\n",
       "      <td>-3.0672</td>\n",
       "      <td>-3.0894</td>\n",
       "      <td>-2.4254</td>\n",
       "      <td>-2.4248</td>\n",
       "      <td>-2.4204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.9622</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.9008</td>\n",
       "      <td>0.9052</td>\n",
       "      <td>0.9106</td>\n",
       "      <td>0.9148</td>\n",
       "      <td>0.9118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 70 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ff86ede-de8f-4b21-ade5-6b84b6030157')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5ff86ede-de8f-4b21-ade5-6b84b6030157 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5ff86ede-de8f-4b21-ade5-6b84b6030157');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           nll_ind                                                 nll_all  \\\n",
       "n             25      50      100     250     500     750     1000    25     \n",
       "model_name                                                                   \n",
       "bae        -2.9085 -2.9041 -2.9014 -2.9369 -2.9919 -3.0142 -3.0332 -2.4143   \n",
       "bma        -2.7185 -2.7150 -2.7012 -2.7662 -2.8025 -2.8372 -2.8615 -2.2770   \n",
       "bne_skew   -2.9609 -2.9492 -2.9585 -3.0208 -3.0611 -3.0660 -3.1046 -2.4606   \n",
       "bne_var    -2.9473 -2.9420 -2.9547 -3.0024 -3.0403 -3.0672 -3.0894 -2.4254   \n",
       "\n",
       "                            ... cov_prob_95_ind                  \\\n",
       "n             50      100   ...            500     750     1000   \n",
       "model_name                  ...                                   \n",
       "bae        -2.4009 -2.3901  ...          0.9602  0.9635  0.9593   \n",
       "bma        -2.2524 -2.2542  ...          0.9491  0.9508  0.9489   \n",
       "bne_skew   -2.4597 -2.4620  ...          0.9691  0.9701  0.9687   \n",
       "bne_var    -2.4248 -2.4204  ...          0.9652  0.9650  0.9622   \n",
       "\n",
       "           cov_prob_95_all                                                  \n",
       "n                     25      50      100     250     500     750     1000  \n",
       "model_name                                                                  \n",
       "bae                 0.8812  0.8828  0.8788  0.8846  0.8958  0.8998  0.8954  \n",
       "bma                 0.8712  0.8704  0.8714  0.8744  0.8864  0.8898  0.8816  \n",
       "bne_skew            0.9344  0.9356  0.9372  0.9438  0.9440  0.9408  0.9422  \n",
       "bne_var             0.8988  0.9030  0.9008  0.9052  0.9106  0.9148  0.9118  \n",
       "\n",
       "[4 rows x 70 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_pd_2d = metric_pd.copy()\n",
    "metric_pd_wide = metric_pd_2d[['model_name', 'n', \n",
    "                               # 'mse_ind',\t\n",
    "                               'nll_ind', 'nll_all',\n",
    "                               'clb_ind', 'clb_all',\n",
    "                               'shp_ind', 'shp_all',\n",
    "                               'ece_ind', 'ece_all',\n",
    "                               'cov_prob_95_ind', 'cov_prob_95_all']]\n",
    "metric_pd_wide.pivot(index='model_name',columns='n').round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTENngJkJryh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FyVOAW4EODnT",
    "ebzyBOEoNQ_a",
    "vAgjEq1-dty-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
