{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickl\\anaconda3\\lib\\site-packages\\gpflow\\experimental\\utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "C:\\Users\\nickl\\anaconda3\\lib\\site-packages\\gpflow\\experimental\\utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0. Expected: 2.7.0\n",
      "TensorFlow Probability version: 0.18.0. Expected: 0.15.0\n"
     ]
    }
   ],
   "source": [
    "from wrapper_functions_CAR import *\n",
    "tf.config.run_functions_eagerly(True)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results and analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CAR_samples', 'chain_samples', 'sampler_stat', 'phi_true', 'u_true', 'data', 'adjacency', 'pivot_fit', 'pivot_DGP', 'models', 'mcmc_config'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#local_dir = 'C:/Users/Admin-Dell/Dropbox/Nick_Cranston/HSPH/Research/Nethery Project/Data/'\n",
    "local_dir = 'C:/Users/nickl/Dropbox/Nick_Cranston/HSPH/Research/Nethery Project/Data/'\n",
    "file = local_dir + 'CAR_samples_NY_n10000_realdata_fl64_2models_June282023.pickle'\n",
    "with open(file, \"rb\") as input_file:\n",
    "     #CAR_samples, chain_samples, sampler_stat, mcmc_config, phi_true, u_true, data_sub = pickle.load(input_file)\n",
    "    res_dict = pickle.load(input_file)\n",
    "res_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the gradient and likelihood values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing tau2 and rho\n",
      "when adding in tau2 and rho, need to update the likelihood function!\n"
     ]
    }
   ],
   "source": [
    "adjacency = res_dict['adjacency']\n",
    "data = res_dict['data']\n",
    "pivot = res_dict['pivot_fit']\n",
    "models = res_dict['models']\n",
    "\n",
    "tau2 = 1\n",
    "rho = 0.3\n",
    "print('fixing tau2 and rho')\n",
    "print('when adding in tau2 and rho, need to update the likelihood function!')\n",
    "\n",
    "Q = (1/tau2)*(np.diag(adjacency.sum(axis=1)) - rho*adjacency)\n",
    "Q = tf.constant(Q, dtype = tf.float64)\n",
    "\n",
    "# define log likelihood function\n",
    "def target_log_prob_fn_CAR(phi, debug_return = False):\n",
    "    #Q = (1/tau2)*(np.diag(adjacency.sum(axis=1)) - rho*adjacency)\n",
    "    #Q = tf.constant(Q, dtype = tf.float64)\n",
    "\n",
    "    ll = tf.Variable(0., dtype = tf.float64)\n",
    "    A = tf.Variable(0., dtype = tf.float64)\n",
    "    B = tf.Variable(0., dtype = tf.float64)\n",
    "    C = tf.Variable(0., dtype = tf.float64)\n",
    "\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        A = A - 0.5*tf.reduce_mean(tf.linalg.diag_part(\n",
    "            tf.linalg.matmul(phi[chain,:,:],tf.linalg.matmul(Q, phi[chain,:,:]), transpose_a = True))) \n",
    "    ll = ll + A\n",
    "\n",
    "    # add in determinant values\n",
    "    # these are multiplied by the number of chains because they are included in the likelihood for each\n",
    "    log_det = tf.constant(np.linalg.slogdet(Q)[1], dtype = tf.float64)\n",
    "    B = 0.5*phi.shape[0]*len(models)*log_det\n",
    "    #log_det = tf.linalg.logdet(Q)[1], dtype = tf.float64\n",
    "    ll = ll + B \n",
    "\n",
    "    if(pivot == -1):\n",
    "        # get exponentiated values and sum across models\n",
    "        exp_phi = tf.math.exp(phi)\n",
    "        exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    elif(pivot in range(3)):\n",
    "        phi_np = phi.numpy()\n",
    "        phi_np = np.insert(phi_np, pivot, 0., axis = 2)\n",
    "        exp_phi = tf.math.exp(tf.constant(phi_np))\n",
    "        exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    else:\n",
    "        raise Exception('Pivot needs to be -1, 0, 1, or 2')\n",
    "\n",
    "    # get model weights and calculate mean estimate\n",
    "    u = tf.math.divide(exp_phi, exp_phi_rows[...,None])\n",
    "\n",
    "    ## Do an (n x m) X (n x m) element wise multiplication\n",
    "    tmp = tf.math.multiply(data[models].values, u)\n",
    "    n = tf.reduce_sum(tmp, axis = 2)\n",
    "\n",
    "    # update the log likelihood \n",
    "    C = tf.reduce_sum([tf.math.reduce_sum(\n",
    "        tf.math.subtract(tf.math.multiply(data['census'],tf.math.log(n[chain,:])),\n",
    "                         n[chain,:])) for chain in range(phi.shape[0])])\n",
    "    ll = ll + C\n",
    "\n",
    "    if debug_return:\n",
    "        dct = {'phi': A, \n",
    "               'det': B,\n",
    "               'Poisson': C,\n",
    "               'total': ll}\n",
    "        return(dct)\n",
    "    else:\n",
    "        return(ll) \n",
    "    \n",
    "def target_log_prob_fn_CAR2(phi, debug_return = False):\n",
    "    #Q = (1/tau2)*(np.diag(adjacency.sum(axis=1)) - rho*adjacency)\n",
    "    #Q = tf.constant(Q, dtype = tf.float64)\n",
    "\n",
    "    ll = tf.Variable(0., dtype = tf.float64)\n",
    "    A = tf.Variable(0., dtype = tf.float64)\n",
    "    B = tf.Variable(0., dtype = tf.float64)\n",
    "    C = tf.Variable(0., dtype = tf.float64)\n",
    "\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        A = A - 0.5*tf.reduce_mean(tf.linalg.diag_part(\n",
    "            tf.linalg.matmul(phi[chain,:,:],tf.linalg.matmul(Q, phi[chain,:,:]), transpose_a = True))) \n",
    "    ll = ll + A\n",
    "\n",
    "    # add in determinant values\n",
    "    # these are multiplied by the number of chains because they are included in the likelihood for each\n",
    "    log_det = tf.linalg.slogdet(Q)[1]\n",
    "    B = 0.5*phi.shape[0]*len(models)*log_det\n",
    "    ll = ll + B \n",
    "\n",
    "    if(pivot == -1):\n",
    "        # get exponentiated values and sum across models\n",
    "        exp_phi = tf.math.exp(phi)\n",
    "        exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    elif(pivot in range(3)):\n",
    "        phi_np = phi.numpy()\n",
    "        phi_np = np.insert(phi_np, pivot, 0., axis = 2)\n",
    "        exp_phi = tf.math.exp(tf.constant(phi_np))\n",
    "        exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    else:\n",
    "        raise Exception('Pivot needs to be -1, 0, 1, or 2')\n",
    "\n",
    "    # get model weights and calculate mean estimate\n",
    "    u = tf.math.divide(exp_phi, exp_phi_rows[...,None])\n",
    "\n",
    "    ## Do an (n x m) X (n x m) element wise multiplication\n",
    "    tmp = tf.math.multiply(data[models].values, u)\n",
    "    n = tf.reduce_sum(tmp, axis = 2)\n",
    "\n",
    "    # update the log likelihood \n",
    "    C = tf.reduce_sum([tf.math.reduce_sum(\n",
    "        tf.math.subtract(tf.math.multiply(data['census'],tf.math.log(n[chain,:])),\n",
    "                         n[chain,:])) for chain in range(phi.shape[0])])\n",
    "    ll = ll + C\n",
    "\n",
    "    if debug_return:\n",
    "        dct = {'phi': A, \n",
    "               'det': B,\n",
    "               'Poisson': C,\n",
    "               'total': ll}\n",
    "        return(dct)\n",
    "    else:\n",
    "        return(ll)  \n",
    "\n",
    "def log_wrapper(phi):\n",
    "    tt = target_log_prob_fn_CAR(phi)\n",
    "    return(tt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing tau2 and rho\n",
      "when adding in tau2 and rho, need to update the likelihood function and confirm gradient works!\n"
     ]
    }
   ],
   "source": [
    "target_log_prob_fn = get_log_prob_from_results(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float64, numpy=1215950602.5388224>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=1215950602.5388224>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=1215950602.5388224>,\n",
       " 1215950602.5388224)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = res_dict['chain_samples'][0]\n",
    "target_log_prob_fn_CAR(phi), target_log_prob_fn(phi), target_log_prob_fn_CAR2(phi), log_wrapper(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(abs(grad1 - grad2))/sum(abs(grad2)) = 0.001360237670822196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.23841858, -0.12397766],\n",
       "        [-0.08106232, -0.19550323],\n",
       "        [-1.31130219,  0.93221664]]),\n",
       " <tf.Tensor: shape=(3, 2), dtype=float64, numpy=\n",
       " array([[ 0.23951625, -0.12441089],\n",
       "        [-0.08057855, -0.19535149],\n",
       "        [-1.31040306,  0.93256642]])>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad1, grad2, diff = gradient_comparison(phi, target_log_prob_fn)\n",
    "grad2[0,:3,:], grad1[0,:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(abs(grad1 - grad2))/sum(abs(grad2)) = 0.001360237670822196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.23841858, -0.12397766],\n",
       "        [-0.08106232, -0.19550323],\n",
       "        [-1.31130219,  0.93221664]]),\n",
       " <tf.Tensor: shape=(3, 2), dtype=float64, numpy=\n",
       " array([[ 0.23951625, -0.12441089],\n",
       "        [-0.08057855, -0.19535149],\n",
       "        [-1.31040306,  0.93256642]])>,\n",
       " 0.5694585164372489,\n",
       " 418.64633560180664)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad1, grad2, diff = gradient_comparison(phi, target_log_prob_fn_CAR2)\n",
    "grad2[0,:3,:], grad1[0,:3,:], np.sum(abs(diff)), np.sum(abs(grad2))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FyVOAW4EODnT",
    "ebzyBOEoNQ_a",
    "vAgjEq1-dty-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
