{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin-Dell\\anaconda3\\lib\\site-packages\\gpflow\\experimental\\utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "C:\\Users\\Admin-Dell\\anaconda3\\lib\\site-packages\\gpflow\\experimental\\utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0. Expected: 2.7.0\n",
      "TensorFlow Probability version: 0.19.0. Expected: 0.15.0\n"
     ]
    }
   ],
   "source": [
    "from wrapper_functions_CAR import *\n",
    "tf.config.run_functions_eagerly(True)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the fake data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training2010 = pd.read_csv('../data/merged_wp_census_data_280922.csv')\n",
    "training2010 = pd.read_csv('../data/merged_wp_census_data2_081122.csv')\n",
    "county_adj = pd.read_csv('../data/countyadj2.csv', index_col = 0)\n",
    "#models = ['acs', 'pep', 'worldpop']\n",
    "models = ['acs', 'pep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(data, adjacency,pivot = -1, sim_numbers = False, one_model = False, models = ['acs', 'pep', 'worldpop']):\n",
    "    \"\"\"Simulated data for the CAR model. \n",
    "    \n",
    "    Args:\n",
    "        data: The input data file.\n",
    "        adjacency: the adjacency matrix for the data.\n",
    "        pivot: The column to be used as the pivot for the softmax ensemble weights. -1 indicates no pivot.\n",
    "        sim_numbers: whether to simulate data values for the models. If false, the true values are used.\n",
    "        one_model: whether to only have one model determine the output. Worldpop is the default if this is chosen.\n",
    "    \"\"\"\n",
    "    if sim_numbers:\n",
    "        data['acs'] = np.random.normal(80.0, 10.0, data.shape[0])\n",
    "        data['pep'] = np.random.normal(100.0, 10.0, data.shape[0])\n",
    "        data['worldpop'] = np.random.normal(120.0, 10.0, data.shape[0])\n",
    "    \n",
    "    if one_model:\n",
    "        data['census'] = np.random.poisson(data['worldpop'].to_numpy())\n",
    "        \n",
    "        return _, _, data\n",
    "    else:\n",
    "        tau2 = 1\n",
    "        rho = 0.3\n",
    "        print('fixing tau2 and rho')\n",
    "        nchain = 1\n",
    "        \n",
    "        Q = (1/tau2)*(np.diag(adjacency.sum(axis=1)) - rho*adjacency)\n",
    "        Q = tf.constant(Q, dtype = tf.float32)\n",
    "\n",
    "        if(pivot == -1):\n",
    "            phi_true = tf.constant(np.array([mv_normal_sample(precision_matrix = Q, \n",
    "                                                          num_models = len(models)) for i in range(nchain)]),\n",
    "                               dtype = tf.float32)\n",
    "\n",
    "        elif(pivot in range(len(models))):\n",
    "            nm = len(models) - 1 \n",
    "            phi_np = np.array([mv_normal_sample(precision_matrix = Q,\n",
    "                                          num_models = nm) for i in range(nchain)])\n",
    "            # fix the added dimension if one is dropped\n",
    "            if nm == 1:\n",
    "                phi_np = phi_np[:,:,np.newaxis]\n",
    "            phi_np = np.insert(phi_np, pivot, 0., axis = 2)\n",
    "            phi_true = tf.constant(phi_np, dtype = tf.float32)\n",
    "\n",
    "        else:\n",
    "            raise Exception('Pivot needs to be -1, 0, 1, or 2')\n",
    "\n",
    "        # get exponentiated values and sum across models\n",
    "        exp_phi = tf.math.exp(phi_true)\n",
    "        exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "\n",
    "        # get model weights and calculate mean estimate\n",
    "        u_true = exp_phi/exp_phi_rows[...,None]\n",
    "\n",
    "        tmp = data[models].values*u_true\n",
    "        n = tf.reduce_sum(tmp, axis = 2)\n",
    "\n",
    "        data['census'] = np.random.poisson(n)[0]\n",
    "\n",
    "    return phi_true, u_true, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_NY, adj_NY = subset_data_by_state(training2010, county_adj, 'New York', 'NY')\n",
    "phi_true, u_true, data = simulate_data(data_NY, adj_NY, pivot = -1, sim_numbers = True, one_model = False, models = models)\n",
    "#data = simulate_data2(data_NY, adj_NY)\n",
    "data_sub = data_NY\n",
    "adj_sub = adj_NY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Nmbq-0wgYy"
   },
   "source": [
    "# Default Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fY1Z6ccs2XUP"
   },
   "outputs": [],
   "source": [
    "# MCMC configs.\n",
    "mcmc_step_size=0.1 # @param\n",
    "mcmc_sample_size=500 # @param\n",
    "mcmc_num_steps=10_000 # @param\n",
    "mcmc_burnin=2_500 # @param\n",
    "mcmc_nchain=10 # @param\n",
    "mcmc_seed=0 # @param\n",
    "\n",
    "DEFAULT_MCMC_CONFIG = dict(step_size=mcmc_step_size, \n",
    "                           num_steps=mcmc_sample_size, \n",
    "                           burnin=mcmc_burnin, \n",
    "                           nchain=mcmc_nchain, \n",
    "                           seed=mcmc_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running with HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_config = DEFAULT_MCMC_CONFIG.copy()\n",
    "mcmc_config.update(dict(burnin = 2_500, num_steps = 10_000, nchain = 5, \n",
    "                        kernel_type = 'hmc', step_adaptor_type = 'simple'))\n",
    "mcmc_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_true, u_true, data_pivot = simulate_data(data_NY, adj_NY, sim_numbers = False,\n",
    "                                                  pivot = -1, one_model = False, models = models)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "CAR_samples, chain_samples, sampler_stat = run_mcmc_CAR(data = data_pivot,\n",
    "                                                        adjacency = adj_NY,\n",
    "                                                        pivot = pivot_fit,\n",
    "                                                        models = models,                                                        \n",
    "                                                        debug_mode = True,\n",
    "                                                        **mcmc_config)  \n",
    "print(time.perf_counter() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for pivot_DGP in range(-1, 2):\n",
    "    print('pivot DGP: ' + str(pivot_DGP))\n",
    "    \n",
    "    phi_true, u_true, data_pivot = simulate_data(data_NY, adj_NY, sim_numbers = False,\n",
    "                                                  pivot = pivot_DGP, one_model = False, models = models)\n",
    "    \n",
    "    for pivot_fit in range(-1, 2):\n",
    "        print('pivot fit: ' + str(pivot_fit))\n",
    "        t0 = time.perf_counter()\n",
    "        CAR_samples, chain_samples, sampler_stat = run_mcmc_CAR(data = data_pivot,\n",
    "                                                                adjacency = adj_NY,\n",
    "                                                                pivot = pivot_fit,\n",
    "                                                                models = models,                                                        \n",
    "                                                                debug_mode = True,\n",
    "                                                                **mcmc_config)  \n",
    "        print(time.perf_counter() - t0)\n",
    "        \n",
    "        res_dict['pDGP: ' + str(pivot_DGP) + '; pfit: ' + str(pivot_fit)] = \n",
    "        [CAR_samples, chain_samples, sampler_stat, phi_true, u_true, data_pivot, pivot_DGP, pivot_fit, models]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading Python objects with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# using local data file because these are too big for github\n",
    "#local_dir = 'C:/Users/nickl/Dropbox/Nick_Cranston/HSPH/Research/Nethery Project/Data/'\n",
    "local_dir = 'C:/Users/Admin-Dell/Dropbox/Nick_Cranston/HSPH/Research/Nethery Project/Data/'\n",
    "with open(local_dir + 'CAR_samples_NY_n10000_realdata_2models_May102023.pickle', 'wb') as results_file:\n",
    "  pickle.dump([res_dict, mcmc_config], results_file)\n",
    "  #pickle.dump([CAR_samples, CAR_samples0, CAR_samples1, CAR_samples2, CAR_samples], results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results and analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = 'C:/Users/Admin-Dell/Dropbox/Nick_Cranston/HSPH/Research/Nethery Project/Data/'\n",
    "file = local_dir + 'CAR_samples_NY_n10000_realdata_2models_May102023.pickle'\n",
    "with open(file, \"rb\") as input_file:\n",
    "     #CAR_samples, chain_samples, sampler_stat, mcmc_config, phi_true, u_true, data_sub = pickle.load(input_file)\n",
    "    res_dict, mcmc_config = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pDGP: -1; pfit: -1\n",
      "105.92601\n",
      "1.0746096\n",
      "pDGP: -1; pfit: 0\n",
      "320.15405\n",
      "1.024241\n",
      "pDGP: -1; pfit: 1\n",
      "297.1085\n",
      "1.0234978\n",
      "pDGP: 0; pfit: -1\n",
      "131.44038\n",
      "1.0577407\n",
      "pDGP: 0; pfit: 0\n",
      "377.65918\n",
      "1.0173694\n",
      "pDGP: 0; pfit: 1\n",
      "300.9055\n",
      "1.0238311\n",
      "pDGP: 1; pfit: -1\n",
      "140.39421\n",
      "1.0487586\n",
      "pDGP: 1; pfit: 0\n",
      "273.1514\n",
      "1.0225117\n",
      "pDGP: 1; pfit: 1\n",
      "270.8465\n",
      "1.0254425\n"
     ]
    }
   ],
   "source": [
    "for key in res_dict.keys():\n",
    "    print(key)\n",
    "    chain_samples = res_dict[key][1]\n",
    "    u_samples = phi_to_u(chain_samples, res_dict[key][7])\n",
    "    ESS = tfp.mcmc.effective_sample_size(u_samples, cross_chain_dims = 1).numpy()\n",
    "    rhat = tfp.mcmc.potential_scale_reduction(chain_samples, independent_chain_ndims=1).numpy()\n",
    "    print(ESS.mean())\n",
    "    print(rhat.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([10000, 5, 62, 1]), TensorShape([62, 1, 500]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_samples = res_dict['pDGP: -1; pfit: 0'][1]\n",
    "CAR_samples = res_dict['pDGP: -1; pfit: 0'][0]\n",
    "chain_samples.shape, CAR_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.insert(chain_samples, 1, 0., axis = 3)\n",
    "chain_samples.shape, test.shape, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "labels = models[:]\n",
    "labels.insert(0, 'none')\n",
    "\n",
    "fig = plt.figure()\n",
    "iter = 0\n",
    "\n",
    "for pivot_DGP in range(-1,2):\n",
    "    \n",
    "    iter = iter + 1\n",
    "    plt.subplot(3, 4, iter)\n",
    "    \n",
    "    u_true = res_dict['pDGP: ' + str(pivot_DGP) + '; pfit: -1'][4]\n",
    "    \n",
    "    CAR_df = pd.DataFrame(u_true[0,:,:], columns = models)\n",
    "    #CAR_df = pd.DataFrame(CAR_ensemble_weights.numpy(), columns = [\"acs\",\"pep\",\"worldpop\"])\n",
    "\n",
    "    plt.subplot(3, 4, iter)\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    tt = sns.kdeplot(CAR_df['acs'], shade=True, color=\"r\", label = 'acs')\n",
    "    tt = sns.kdeplot(CAR_df[\"pep\"], shade=True, color=\"b\", label = 'pep')\n",
    "    #tt = sns.kdeplot(CAR_df[\"worldpop\"], shade=True, color=\"g\", label = 'worldpop')\n",
    "    if pivot_DGP == -1:\n",
    "        plt.title('u true')\n",
    "    plt.ylabel(labels[pivot_DGP + 1])\n",
    "    plt.xlabel('')\n",
    "    plt.xlim(0,1)\n",
    "    \n",
    "    for pivot_fit in range(-1,2):\n",
    "        iter = iter + 1\n",
    "        key = 'pDGP: ' + str(pivot_DGP) + '; pfit: ' + str(pivot_fit)    \n",
    "        #print(key)\n",
    "        CAR_samples = res_dict[key][0]\n",
    "        CAR_ensemble_weights = tf.reduce_mean(CAR_samples[0], axis = 2).numpy()\n",
    "        if pivot_fit > -1:\n",
    "            CAR_ensemble_weights = tf.constant(np.insert(CAR_ensemble_weights, pivot_fit, 0., axis = 1))\n",
    "        #CAR_ensemble_weights = tf.reduce_mean(CAR_samples[0], axis = 2)\n",
    "\n",
    "        # get exponentiated values and sum across models\n",
    "        exp_phi = tf.math.exp(CAR_ensemble_weights)\n",
    "\n",
    "        exp_phi_rows = tf.reduce_sum(exp_phi, 1)\n",
    "\n",
    "        # get model weights and calculate mean estimate\n",
    "        u = exp_phi/exp_phi_rows[...,None]\n",
    "\n",
    "        CAR_df = pd.DataFrame(u, columns = models)\n",
    "        #CAR_df = pd.DataFrame(CAR_ensemble_weights.numpy(), columns = [\"acs\",\"pep\",\"worldpop\"])\n",
    "\n",
    "        plt.subplot(3, 4, iter)\n",
    "        sns.set(style=\"darkgrid\")\n",
    "        tt = sns.kdeplot(CAR_df['acs'], shade=True, color=\"r\", label = 'acs')\n",
    "        tt = sns.kdeplot(CAR_df[\"pep\"], shade=True, color=\"b\", label = 'pep')\n",
    "        #tt = sns.kdeplot(CAR_df[\"worldpop\"], shade=True, color=\"g\", label = 'worldpop')\n",
    "        #plt.legend()\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        if pivot_DGP == -1:\n",
    "            plt.title(labels[pivot_fit + 1])\n",
    "        plt.xlim(0,1)\n",
    "        \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the average y predictions from the MCMC samples\n",
    "\n",
    "pivot = 0\n",
    "predictions = np.empty(shape=(CAR_samples[0].shape[0], CAR_samples[0].shape[2]), dtype='object')\n",
    "for i in range(CAR_samples[0].shape[2]):\n",
    "    phi_vals = CAR_samples[0][:,:,i].numpy()\n",
    "    phi_vals = np.insert(phi_vals, pivot, 0., axis = 1)\n",
    "    \n",
    "    # get exponentiated values and sum across models\n",
    "    exp_phi = tf.math.exp(tf.constant(phi_vals))\n",
    "\n",
    "    exp_phi_rows = tf.reduce_sum(exp_phi, 1)\n",
    "\n",
    "    # get model weights and calculate mean estimate\n",
    "    u = exp_phi/exp_phi_rows[...,None]\n",
    "\n",
    "    tmp = data_sub[models].values*u\n",
    "    predictions[:,i] = tf.reduce_sum(tmp, axis = 1)\n",
    "    \n",
    "y_pred = np.mean(predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the average ensemble weights from the CAR samples\n",
    "\n",
    "CAR_ensemble_weights = tf.reduce_mean(CAR_samples[0], axis = 2).numpy()\n",
    "CAR_ensemble_weights = tf.constant(np.insert(CAR_ensemble_weights, pivot, 0., axis = 1))\n",
    "#CAR_ensemble_weights = tf.reduce_mean(CAR_samples[0], axis = 2)\n",
    "\n",
    "# get exponentiated values and sum across models\n",
    "exp_phi = tf.math.exp(CAR_ensemble_weights)\n",
    "\n",
    "exp_phi_rows = tf.reduce_sum(exp_phi, 1)\n",
    "    \n",
    "# get model weights and calculate mean estimate\n",
    "u = exp_phi/exp_phi_rows[...,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the weights dict for plotting the outcomes\n",
    "\n",
    "weights_dict = {\n",
    "    \"acs\": CAR_ensemble_weights[:,0],\n",
    "    \"pep\": CAR_ensemble_weights[:,1],\n",
    "    \"worldpop\": CAR_ensemble_weights[:,2]\n",
    "}\n",
    "\n",
    "color_weights = make_color_norm(\n",
    "    list(weights_dict.values())[1],   \n",
    "    method=\"percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(data_sub['census'],data_sub['pep'])[0], pearsonr(data_sub['census'],data_sub['acs'])[0], pearsonr(data_sub['census'],data_sub['worldpop'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "CAR_ensemble_weights = tf.reduce_mean(CAR_samples[0], axis = 2).numpy()\n",
    "CAR_ensemble_weights = np.insert(CAR_ensemble_weights, pivot, 0., axis = 1)\n",
    "\n",
    "CAR_df = pd.DataFrame(CAR_ensemble_weights, columns = [\"acs\",\"pep\",\"worldpop\"])\n",
    "#CAR_df = pd.DataFrame(CAR_ensemble_weights.numpy(), columns = [\"acs\",\"pep\",\"worldpop\"])\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig = sns.kdeplot(CAR_df['acs'], shade=True, color=\"r\", label = 'acs')\n",
    "fig = sns.kdeplot(CAR_df[\"pep\"], shade=True, color=\"b\", label = 'pep')\n",
    "fig = sns.kdeplot(CAR_df[\"worldpop\"], shade=True, color=\"g\", label = 'worldpop')\n",
    "plt.legend()\n",
    "plt.xlabel('phi values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "CAR_df = pd.DataFrame(u, columns = [\"acs\",\"pep\",\"worldpop\"])\n",
    "#CAR_df = pd.DataFrame(CAR_ensemble_weights.numpy(), columns = [\"acs\",\"pep\",\"worldpop\"])\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig = sns.kdeplot(CAR_df['acs'], shade=True, color=\"r\", label = 'acs')\n",
    "fig = sns.kdeplot(CAR_df[\"pep\"], shade=True, color=\"b\", label = 'pep')\n",
    "fig = sns.kdeplot(CAR_df[\"worldpop\"], shade=True, color=\"g\", label = 'worldpop')\n",
    "plt.legend()\n",
    "plt.xlabel('u values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get exponentiated values and sum across models\n",
    "exp_phi = tf.math.exp(CAR_ensemble_weights)\n",
    "exp_phi_rows = tf.reduce_sum(exp_phi, 1)\n",
    "\n",
    "# get model weights and calculate mean estimate\n",
    "u = exp_phi/exp_phi_rows[...,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_weights_dict = {\n",
    "    \"acs\": u[:,0],\n",
    "    \"pep\": u[:,1],\n",
    "    \"worldpop\": u[:,2]\n",
    "}\n",
    "\n",
    "color_norm_weights = make_color_norm(\n",
    "    list(norm_weights_dict.values())[1],   \n",
    "    method=\"percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "\n",
    "import pandas as pd\n",
    "# df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/fips-unemp-16.csv\",\n",
    "#                    dtype={\"fips\": str})\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    output = pd.DataFrame(np.column_stack([data_sub[[\"GEOID\"]], weights_dict[model_name]]))\n",
    "    output = output.set_axis(['GEOID', model_name], axis=1)\n",
    "    output[model_name] = output[model_name].astype(float)\n",
    "    fig = px.choropleth_mapbox(output, geojson=counties, locations='GEOID', color=model_name,\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           #range_color=(0.05,0.07),\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           #mapbox_style='white-bg',\n",
    "                           #featureidkey=\"properties.MWS_ID\",\n",
    "                           zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n",
    "                           opacity=0.5#,\n",
    "                           #labels={'unemp':'unemployment rate'}\n",
    "                          )\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([data_NY[[\"GEOID\"]].reset_index(drop=True), pd.DataFrame( phi_true[0].numpy(), columns = models)], axis=1)\n",
    "# pd.DataFrame( phi_true[0].numpy(), columns = models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_phi = pd.concat([data_sub[[\"GEOID\"]].reset_index(drop=True), \n",
    "                         pd.DataFrame(phi_true[0].numpy(), columns = models)], \n",
    "                        axis=1)\n",
    "\n",
    "full_u = pd.concat([data_sub[[\"GEOID\"]].reset_index(drop=True), \n",
    "                         pd.DataFrame(u_true[0].numpy(), columns = models)], \n",
    "                        axis=1)\n",
    "\n",
    "\n",
    "#full_phi[['GEOID', 'acs']], full_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    output = full_phi[['GEOID', model_name]]\n",
    "    fig = px.choropleth_mapbox(output, geojson=counties, locations='GEOID', color=model_name,\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           #range_color=(0.05,0.07),\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           #mapbox_style='white-bg',\n",
    "                           #featureidkey=\"properties.MWS_ID\",\n",
    "                           zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n",
    "                           opacity=0.5#,\n",
    "                           #labels={'unemp':'unemployment rate'}\n",
    "                          )\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ndarray.flatten(full_phi.iloc[:,1:].to_numpy())\n",
    "y = np.ndarray.flatten(CAR_ensemble_weights.numpy())\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.xlabel(\"true phi\")\n",
    "plt.ylabel(\"mean MCMC phi\")\n",
    "\n",
    "#obtain m (slope) and b(intercept) of linear regression line\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "plt.xlim([min([min(x), min(y)]), max([max(x), max(y)])])\n",
    "plt.ylim([min([min(x), min(y)]), max([max(x), max(y)])])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "#add linear regression line to scatterplot \n",
    "plt.plot(x, m*x+b, color = 'red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ndarray.flatten(full_u.iloc[:,1:].values)\n",
    "y = np.ndarray.flatten(u.numpy())\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"true u\")\n",
    "plt.ylabel(\"mean MCMC u\")\n",
    "\n",
    "#obtain m (slope) and b(intercept) of linear regression line\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "plt.xlim([min([min(x), min(y)]), max([max(x), max(y)])])\n",
    "plt.ylim([min([min(x), min(y)]), max([max(x), max(y)])])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "#add linear regression line to scatterplot \n",
    "plt.plot(x, m*x+b, color = 'red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_sub['census']\n",
    "y = y_pred\n",
    "#y = y_pred.numpy()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"true census values\")\n",
    "plt.ylabel(\"predicted census values\")\n",
    "\n",
    "#obtain m (slope) and b(intercept) of linear regression line\n",
    "m, b = np.polyfit(x, y.astype('float64'), 1)\n",
    "\n",
    "plt.xlim([min([min(x), min(y)]), max([max(x), max(y)])])\n",
    "plt.ylim([min([min(x), min(y)]), max([max(x), max(y)])])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "#add linear regression line to scatterplot \n",
    "plt.plot(x, m*x+b, color = 'red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(x,y)[0]"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FyVOAW4EODnT",
    "ebzyBOEoNQ_a",
    "vAgjEq1-dty-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
