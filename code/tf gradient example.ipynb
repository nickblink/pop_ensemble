{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d9d9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e89a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "training2010 = pd.read_csv('../data/merged_wp_census_data2_081122.csv')\n",
    "training2010=training2010.fillna(0)\n",
    "county_adj = pd.read_csv('../data/countyadj2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c6e6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function was taken from online\n",
    "# Generate samples from a multi-variate normal distribution with provided precision matrix WITHOUT inverting\n",
    "def mv_normal_sample(mu=0, precision_matrix=None, num_models=1):\n",
    "\n",
    "    # Precision matrix must be a square matrix\n",
    "    assert precision_matrix.shape[0] == precision_matrix.shape[1], 'Precision matrix must be a square matrix'\n",
    "\n",
    "    dim = precision_matrix.shape[0]\n",
    "\n",
    "    chol_U = scipy.linalg.cholesky(precision_matrix, lower=False)\n",
    "\n",
    "    # Create num_models iid standard normal vectors\n",
    "    z_vector_matrix = np.random.normal(loc=0, scale=1, size=[num_models, dim])\n",
    "\n",
    "    # Sample from the MV normal with precision matrix by solving the Cholesky decomp for each normal vector\n",
    "    samples = np.squeeze(np.array(\n",
    "        [scipy.linalg.solve_triangular(a=chol_U, b=z_vector_matrix[i, :], unit_diagonal=False) + mu for i in\n",
    "         range(num_models)]))\n",
    "\n",
    "    return (np.transpose(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5ee3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchain = 5\n",
    "tau2 = 100\n",
    "rho = 0.3\n",
    "\n",
    "Q = (1/tau2)*(np.diag(county_adj.sum(axis=1)) - rho*county_adj)\n",
    "Q = tf.constant(Q, dtype = tf.float32)\n",
    "\n",
    "init_state = tf.constant(np.array([mv_normal_sample(precision_matrix = Q, num_models = 3) for i in range(nchain)]),\n",
    "                        dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7692e",
   "metadata": {},
   "source": [
    "This gradient below works fine since it uses the tensorflow mean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31e77815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3064,), dtype=float32, numpy=\n",
       "array([-0.3884592,  4.1239996, -5.3311143, ...,  6.8933125,  0.7996433,\n",
       "       -8.623241 ], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = init_state[0,:,:]\n",
    "tf.math.reduce_sum(phi, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f7f1258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.00010879 0.00010879 0.00010879]\n",
      " [0.00010879 0.00010879 0.00010879]\n",
      " [0.00010879 0.00010879 0.00010879]\n",
      " ...\n",
      " [0.00010879 0.00010879 0.00010879]\n",
      " [0.00010879 0.00010879 0.00010879]\n",
      " [0.00010879 0.00010879 0.00010879]], shape=(3064, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(phi)\n",
    "  y = tf.reduce_mean(phi)\n",
    "dy_dx = g.gradient(y,phi)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1158a69",
   "metadata": {},
   "source": [
    "This gradient below does not work because it uses the numpy mean function, even though it converts the answer to a tf.Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1097337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(phi)\n",
    "  y = tf.constant(np.mean(phi))\n",
    "dy_dx = g.gradient(y,phi)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f5a81",
   "metadata": {},
   "source": [
    "Below is the loss function that I want to use for an MCMC sampler. This function is the log likelihood of my model. Since the operations are not written in tensorflow the gradient doesn't work. I want to know how to implement this in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ecf07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['acs', 'pep', 'worldpop']\n",
    "\n",
    "nchain = 5\n",
    "tau2 = 100\n",
    "rho = 0.3\n",
    "\n",
    "Q = (1/tau2)*(np.diag(county_adj.sum(axis=1)) - rho*county_adj)\n",
    "Q = tf.constant(Q, dtype = tf.float32)\n",
    "\n",
    "def target_log_prob_fn_CAR(phi):\n",
    "    \n",
    "    Q = (1/tau2)*(np.diag(county_adj.sum(axis=1)) - rho_s*county_adj)\n",
    "    Q = tf.constant(Q, dtype = tf.float32)\n",
    "        \n",
    "    ll = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        ll_chain = -0.5*tf.reduce_mean(tf.linalg.diag_part(\n",
    "            tf.linalg.matmul(phi[chain,:,:],tf.linalg.matmul(Q, phi[chain,:,:]), transpose_a = True))) \n",
    "        ll = ll + ll_chain\n",
    "    \n",
    "    # add in determinant values\n",
    "    ll = ll + 0.5*phi.shape[0]*len(models)*np.linalg.slogdet(Q)[1]\n",
    "    \n",
    "    # get exponentiated values and sum across models\n",
    "    exp_phi = tf.math.exp(phi)\n",
    "    exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    \n",
    "    # get model weights and calculate mean estimate\n",
    "    u = exp_phi/exp_phi_rows[...,None]\n",
    "      \n",
    "    tmp = training2010[models].values*u\n",
    "    n = tf.reduce_sum(tmp, axis = 2)\n",
    "    \n",
    "    # update the log likelihood \n",
    "    ll = ll + tf.reduce_sum([np.sum(training2010['census']*np.log(n[chain,:]) - n[chain,:]) for chain in range(phi.shape[0])])\n",
    "    \n",
    "    return(ll)\n",
    "\n",
    "def target_log_prob_fn_test1(phi):\n",
    "    \n",
    "    Q = (1/tau2)*(np.diag(county_adj.sum(axis=1)) - rho_s*county_adj)\n",
    "    Q = tf.constant(Q, dtype = tf.float32)\n",
    "        \n",
    "    ll = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        ll_chain = -0.5*tf.reduce_mean(tf.linalg.diag_part(\n",
    "            tf.linalg.matmul(phi[chain,:,:],tf.linalg.matmul(Q, phi[chain,:,:]), transpose_a = True))) \n",
    "        ll = ll + ll_chain\n",
    "    \n",
    "    # get exponentiated values and sum across models\n",
    "    exp_phi = tf.math.exp(phi)\n",
    "    exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    \n",
    "    # get model weights and calculate mean estimate\n",
    "    u = exp_phi/exp_phi_rows[...,None]\n",
    "      \n",
    "    tmp = training2010[models].values*u\n",
    "    n = tf.reduce_sum(tmp, axis = 2)\n",
    "    \n",
    "    # update the log likelihood \n",
    "    ll = ll + tf.reduce_sum([np.sum(training2010['census']*np.log(n[chain,:]) - n[chain,:]) for chain in range(phi.shape[0])])\n",
    "    \n",
    "    return(ll)\n",
    "\n",
    "def target_log_prob_fn_test2(phi):\n",
    "    \n",
    "    ll = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        ll_chain = -0.5*tf.reduce_mean(tf.linalg.diag_part(\n",
    "            tf.linalg.matmul(phi[chain,:,:],tf.linalg.matmul(Q, phi[chain,:,:]), transpose_a = True))) \n",
    "        ll = ll + ll_chain\n",
    "    \n",
    "    # get exponentiated values and sum across models\n",
    "    exp_phi = tf.math.exp(phi)\n",
    "    exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    \n",
    "    # get model weights and calculate mean estimate\n",
    "    u = exp_phi/exp_phi_rows[...,None]\n",
    "      \n",
    "    tmp = training2010[models].values*u\n",
    "    n = tf.reduce_sum(tmp, axis = 2)\n",
    "    \n",
    "    # update the log likelihood \n",
    "    ll = ll + tf.reduce_sum([np.sum(training2010['census']*np.log(n[chain,:]) - n[chain,:]) for chain in range(phi.shape[0])])\n",
    "    \n",
    "    return(ll)\n",
    "\n",
    "def target_log_prob_fn_test3(phi):\n",
    "    \n",
    "    ll = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        ll_chain = -0.5*tf.reduce_mean(tf.linalg.diag_part(\n",
    "            tf.linalg.matmul(phi[chain,:,:],tf.linalg.matmul(Q, phi[chain,:,:]), transpose_a = True))) \n",
    "        ll = ll + ll_chain\n",
    "    \n",
    "    # get exponentiated values and sum across models\n",
    "    exp_phi = tf.math.exp(phi)\n",
    "    exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    \n",
    "    # get model weights and calculate mean estimate\n",
    "    u = exp_phi/exp_phi_rows[...,None]\n",
    "      \n",
    "    tmp = training2010[models].values*u\n",
    "    n = tf.reduce_sum(tmp, axis = 2)\n",
    "    \n",
    "    # update the log likelihood \n",
    "    ll = ll + tf.reduce_sum([np.sum(training2010['census']*np.log(n[chain,:]) - n[chain,:]) for chain in range(phi.shape[0])])\n",
    "    \n",
    "    return(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72ec789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43322470000021\n",
      "1.8067728999999417\n",
      "0.2740093999996134\n",
      "0.2710925000001225\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "for i in range(10):\n",
    "    a = target_log_prob_fn_CAR(init_state)\n",
    "print(time.perf_counter() - t0)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "for i in range(10):\n",
    "    b = target_log_prob_fn_test1(init_state)\n",
    "print(time.perf_counter() - t0)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "for i in range(10):\n",
    "    c = target_log_prob_fn_test2(init_state)\n",
    "print(time.perf_counter() - t0)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "for i in range(10):\n",
    "    d = target_log_prob_fn_test3(init_state)\n",
    "print(time.perf_counter() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19d222",
   "metadata": {},
   "source": [
    "So the majority of the work (63%) is in the slogdet function. Is there a faster way to do this? Not sure how this works. Is this necessary to compute each time? Is there a way to store the values from successive calls? How can I set a global counter of calls? That would be nice to do. Like I can store the values for the same tau2 and rho. That way I won't have to repeat call this function, because it is uneccessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57a6894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(17883193000.0, shape=(), dtype=float32)\n",
      "tf.Tensor(17883263000.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "375651e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.06293806 -0.19838522 -0.12657136]\n",
      "  [-0.03091805 -0.04359809 -0.19169998]\n",
      "  [ 0.13187447  0.18273549 -0.19368437]\n",
      "  ...\n",
      "  [ 0.029387   -0.04181785 -0.03695919]\n",
      "  [-0.16595784 -0.05239505  0.03625642]\n",
      "  [-0.0399273   0.01138137 -0.00183557]]\n",
      "\n",
      " [[ 0.14912575  0.12052111  0.07781777]\n",
      "  [-0.10916156 -0.07026157  0.14019117]\n",
      "  [-0.04505573  0.09033561  0.10722545]\n",
      "  ...\n",
      "  [ 0.06008033 -0.09975737  0.00951293]\n",
      "  [ 0.00195231  0.00384467  0.10106154]\n",
      "  [ 0.1319578   0.08323511  0.01198121]]\n",
      "\n",
      " [[-0.06555098 -0.02694318  0.04660968]\n",
      "  [ 0.07939043  0.03178913  0.15002967]\n",
      "  [-0.00882181  0.11389777  0.07034828]\n",
      "  ...\n",
      "  [-0.09443994 -0.04512857 -0.00781568]\n",
      "  [-0.00819376 -0.12262242 -0.13971454]\n",
      "  [-0.03177919 -0.09681055 -0.14664814]]\n",
      "\n",
      " [[ 0.08906697  0.0099615  -0.02503883]\n",
      "  [-0.14669773 -0.06447905  0.08280015]\n",
      "  [ 0.05262746 -0.07116685 -0.06322917]\n",
      "  ...\n",
      "  [ 0.00870429  0.01221565 -0.00885389]\n",
      "  [-0.01572543  0.03894701 -0.03172914]\n",
      "  [ 0.13984008  0.11564143 -0.06373934]]\n",
      "\n",
      " [[-0.07396241 -0.04182213  0.14876257]\n",
      "  [ 0.04640286 -0.18161958 -0.02505883]\n",
      "  [ 0.11203246  0.206117   -0.01529706]\n",
      "  ...\n",
      "  [-0.17760348  0.08257662 -0.01694355]\n",
      "  [ 0.09226464  0.10182965  0.09030198]\n",
      "  [-0.00132687  0.11147161  0.02780235]]], shape=(5, 3064, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(init_state)\n",
    "  y = target_log_prob_fn_CAR(init_state)\n",
    "dy_dx = g.gradient(y, init_state)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32f0a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.09127215 -0.20940161 -0.11793244]\n",
      "  [-0.01927063 -0.04018708 -0.19272867]\n",
      "  [ 0.11517505  0.18378654 -0.19533205]\n",
      "  ...\n",
      "  [ 0.03702173 -0.05179448 -0.00932288]\n",
      "  [-0.17377016 -0.0441881   0.00396088]\n",
      "  [-0.04036826 -0.00908633 -0.02932849]]\n",
      "\n",
      " [[ 0.15043935  0.14772579  0.06314655]\n",
      "  [-0.07581358 -0.06281566  0.13385895]\n",
      "  [-0.08030345  0.07824104  0.11501669]\n",
      "  ...\n",
      "  [ 0.06758705 -0.08850935  0.01304885]\n",
      "  [ 0.00876922 -0.0262132   0.0916976 ]\n",
      "  [ 0.15009926  0.09006016 -0.02536286]]\n",
      "\n",
      " [[-0.11102764 -0.00179667  0.05647185]\n",
      "  [ 0.05864335  0.03995913  0.14371407]\n",
      "  [ 0.02571499  0.11598949  0.07710124]\n",
      "  ...\n",
      "  [-0.10117067 -0.04165079 -0.03552025]\n",
      "  [-0.00370917 -0.10642684 -0.10120192]\n",
      "  [-0.01930687 -0.10010205 -0.21290722]]\n",
      "\n",
      " [[ 0.06365295 -0.00117846 -0.00061857]\n",
      "  [-0.1136101  -0.0911833   0.11023029]\n",
      "  [ 0.05521546 -0.04533196 -0.01646566]\n",
      "  ...\n",
      "  [ 0.02088382  0.01095701 -0.0116246 ]\n",
      "  [-0.0087244   0.0754048  -0.03407851]\n",
      "  [ 0.0948333   0.13605219 -0.05307309]]\n",
      "\n",
      " [[-0.06129812  0.00222694  0.0925322 ]\n",
      "  [ 0.0439827  -0.12564969 -0.03192715]\n",
      "  [ 0.06111245  0.18424334 -0.07527876]\n",
      "  ...\n",
      "  [-0.17917936  0.08472731 -0.02656582]\n",
      "  [ 0.09054373  0.0784818   0.08321984]\n",
      "  [-0.02758554  0.10590129 -0.00066238]]], shape=(5, 3064, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(init_state)\n",
    "  y = target_log_prob_fn_test3(init_state)\n",
    "dy_dx = g.gradient(y, init_state)\n",
    "print(dy_dx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
