{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9d9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e89a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "training2010 = pd.read_csv('../data/merged_wp_census_data2_081122.csv')\n",
    "training2010=training2010.fillna(0)\n",
    "county_adj = pd.read_csv('../data/countyadj2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6e6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function was taken from online\n",
    "# Generate samples from a multi-variate normal distribution with provided precision matrix WITHOUT inverting\n",
    "def mv_normal_sample(mu=0, precision_matrix=None, num_models=1):\n",
    "\n",
    "    # Precision matrix must be a square matrix\n",
    "    assert precision_matrix.shape[0] == precision_matrix.shape[1], 'Precision matrix must be a square matrix'\n",
    "\n",
    "    dim = precision_matrix.shape[0]\n",
    "\n",
    "    chol_U = scipy.linalg.cholesky(precision_matrix, lower=False)\n",
    "\n",
    "    # Create num_models iid standard normal vectors\n",
    "    z_vector_matrix = np.random.normal(loc=0, scale=1, size=[num_models, dim])\n",
    "\n",
    "    # Sample from the MV normal with precision matrix by solving the Cholesky decomp for each normal vector\n",
    "    samples = np.squeeze(np.array(\n",
    "        [scipy.linalg.solve_triangular(a=chol_U, b=z_vector_matrix[i, :], unit_diagonal=False) + mu for i in\n",
    "         range(num_models)]))\n",
    "\n",
    "    return (np.transpose(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ee3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchain = 5\n",
    "tau2 = 100\n",
    "rho = 0.3\n",
    "\n",
    "Q = (1/tau2)*(np.diag(county_adj.sum(axis=1)) - rho*county_adj)\n",
    "Q = tf.constant(Q, dtype = tf.float32)\n",
    "\n",
    "init_state = tf.constant(np.array([mv_normal_sample(precision_matrix = Q, num_models = 3) for i in range(nchain)]),\n",
    "                        dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7692e",
   "metadata": {},
   "source": [
    "This gradient below works fine since it uses the tensorflow mean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e77815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3064,), dtype=float32, numpy=\n",
       "array([11.174502 , -2.774537 ,  4.1983414, ...,  1.0655003,  2.7233696,\n",
       "        1.2552919], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = init_state[0,:,:]\n",
    "tf.math.reduce_sum(phi, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7f1258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.00010879 0.00010879 0.00010879]\n",
      " [0.00010879 0.00010879 0.00010879]\n",
      " [0.00010879 0.00010879 0.00010879]\n",
      " ...\n",
      " [0.00010879 0.00010879 0.00010879]\n",
      " [0.00010879 0.00010879 0.00010879]\n",
      " [0.00010879 0.00010879 0.00010879]], shape=(3064, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(phi)\n",
    "  y = tf.reduce_mean(phi)\n",
    "dy_dx = g.gradient(y,phi)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1158a69",
   "metadata": {},
   "source": [
    "This gradient below does not work because it uses the numpy mean function, even though it converts the answer to a tf.Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1097337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(phi)\n",
    "  y = tf.constant(np.mean(phi))\n",
    "dy_dx = g.gradient(y,phi)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f5a81",
   "metadata": {},
   "source": [
    "Below is the loss function that I want to use for an MCMC sampler. This function is the log likelihood of my model. Since the operations are not written in tensorflow the gradient doesn't work. I want to know how to implement this in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecf07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['acs', 'pep', 'worldpop']\n",
    "def target_log_prob_fn_CAR_old(phi):   \n",
    "    ll_chains = []\n",
    "    tmp = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        phi_chain = phi[chain,:,:]\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        ll = -0.5*sum(np.diagonal(np.transpose(phi_chain) @ Q @ phi_chain))\n",
    "\n",
    "        # (2) Prob of observed data points\n",
    "        exp_phi = np.exp(phi_chain)\n",
    "        u = [exp_phi[i,:]/np.sum(exp_phi[i,:]) for i in range(exp_phi.shape[0])]\n",
    "        tmp = training2010[models].values*u\n",
    "        n = tmp.sum(axis=1)\n",
    "        ll = ll + np.sum(training2010['census']*np.log(n) - n)\n",
    "        ll_chains.append(ll)\n",
    "        tmp = tmp + ll\n",
    "    \n",
    "    print(ll_chains)\n",
    "    #return tf.reduce_mean(ll_chains)\n",
    "    return(tmp)\n",
    "\n",
    "def target_log_prob_fn_CAR(phi):\n",
    "    \n",
    "    ll = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        ll_chain = -0.5*tf.reduce_mean(tf.linalg.diag_part(\n",
    "            tf.linalg.matmul(phi[chain,:,:],tf.linalg.matmul(Q, phi[chain,:,:]), transpose_a = True))) \n",
    "        ll = ll + ll_chain\n",
    "        \n",
    "        # (2) Prob of observed data points\n",
    "        exp_phi = tf.math.exp(phi[chain,:,:])\n",
    "        u = [exp_phi[i,:]/tf.reduce_sum(exp_phi[i,:]) for i in range(exp_phi.shape[0])]\n",
    "        tmp = training2010[models].values*u\n",
    "        n = tmp.sum(axis=1)\n",
    "        ll = ll + np.sum(training2010['census']*np.log(n) - n)\n",
    "\n",
    "    return(ll)\n",
    "\n",
    "target_log_prob_fn_CAR(init_state)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(init_state)\n",
    "  y = target_log_prob_fn_CAR(init_state)\n",
    "dy_dx = g.gradient(y, init_state)\n",
    "#print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24bb1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_log_prob_fn_CAR_test1(phi):\n",
    "    \n",
    "    ll = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (1) Prob of the CAR random effect values\n",
    "        ll_chain = -0.5*tf.reduce_mean(tf.linalg.diag_part(\n",
    "            tf.linalg.matmul(phi[chain,:,:],tf.linalg.matmul(Q, phi[chain,:,:]), transpose_a = True))) \n",
    "        ll = ll + ll_chain\n",
    "\n",
    "    return(ll)\n",
    "\n",
    "def target_log_prob_fn_CAR_test2(phi):\n",
    "    \n",
    "    ll = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (2) Prob of observed data points\n",
    "        exp_phi = tf.math.exp(phi[chain,:,:])\n",
    "        u = [exp_phi[i,:]/tf.reduce_sum(exp_phi[i,:]) for i in range(exp_phi.shape[0])]\n",
    "        tmp = training2010[models].values*u\n",
    "        n = tmp.sum(axis=1)\n",
    "        ll = ll + np.sum(training2010['census']*np.log(n) - n)\n",
    "\n",
    "    return(ll)\n",
    "\n",
    "def target_log_prob_fn_CAR_test3(phi):\n",
    "    \n",
    "    ll = tf.Variable(0.)\n",
    "    for chain in range(phi.shape[0]):\n",
    "        # (2) Prob of observed data points\n",
    "        exp_phi = tf.math.exp(phi[chain,:,:])\n",
    "        u = [exp_phi[i,:]/tf.reduce_sum(exp_phi[i,:]) for i in range(exp_phi.shape[0])]\n",
    "        #tmp = training2010[models].values*u\n",
    "        #n = tmp.sum(axis=1)\n",
    "        #ll = ll + np.sum(training2010['census']*np.log(n) - n)\n",
    "\n",
    "    return(ll)\n",
    "\n",
    "def target_log_prob_fn_CAR_test4(phi):\n",
    "    \n",
    "    # get exponentiated values and sum across models\n",
    "    exp_phi = tf.math.exp(phi)\n",
    "    exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "    \n",
    "    # get model weights and calculate mean estimate\n",
    "    u = exp_phi/exp_phi_rows[...,None]\n",
    "    n = tf.reduce_sum(training2010[models].values*u, axis = 2)\n",
    "    \n",
    "    # update the log likelihood \n",
    "    ll = ll + tf.reduce_sum([np.sum(training2010['census']*np.log(n[chain,:]) - n[chain,:]) for chain in range(phi.shape[0])])\n",
    "    \n",
    "    return(ll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "72ec789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.912102099999174\n",
      "6.866808200000378\n",
      "0.012330500001553446\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "a = target_log_prob_fn_CAR_test2(init_state)\n",
    "print(time.perf_counter() - t0)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "b = target_log_prob_fn_CAR_test3(init_state)\n",
    "print(time.perf_counter() - t0)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "c = target_log_prob_fn_CAR_test4(init_state)\n",
    "print(time.perf_counter() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "57a6894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(17883271000.0, shape=(), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n",
      "tf.Tensor(17883271000.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b69be096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007726000003458466\n",
      "1.1622198000004573\n"
     ]
    }
   ],
   "source": [
    "exp_phi = tf.math.exp(init_state)\n",
    "exp_phi_rows = tf.reduce_sum(exp_phi, 2)\n",
    "t0 = time.perf_counter()\n",
    "u1 = exp_phi/exp_phi_rows[...,None]\n",
    "print(time.perf_counter() - t0)\n",
    "t0 = time.perf_counter()\n",
    "u = [exp_phi[0,i,:]/exp_phi_rows[0, i] for i in range(exp_phi.shape[1])]\n",
    "print(time.perf_counter() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9803778",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = training2010[models].values*u\n",
    "tmp2 = training2010[models].values*u1[0,:,:]\n",
    "tmp3 = training2010[models].values*u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0baa36a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3064,), dtype=float32, numpy=\n",
       "array([ 53155.723, 176339.84 ,  27698.83 , ...,  20967.383,   8271.89 ,\n",
       "         7212.482], dtype=float32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tmp2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5c07b776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3064), dtype=float32, numpy=\n",
       "array([[ 53155.723 , 176339.84  ,  27698.83  , ...,  20967.383 ,\n",
       "          8271.89  ,   7212.482 ],\n",
       "       [ 54772.88  , 176398.98  ,  27696.695 , ...,  20636.373 ,\n",
       "          8521.852 ,   7093.7915],\n",
       "       [ 55894.582 , 175792.12  ,  27691.943 , ...,  20825.73  ,\n",
       "          8271.004 ,   7191.672 ],\n",
       "       [ 53452.742 , 175886.16  ,  27395.645 , ...,  20537.242 ,\n",
       "          8527.322 ,   7070.585 ],\n",
       "       [ 54672.77  , 175871.31  ,  27698.883 , ...,  20967.684 ,\n",
       "          8511.356 ,   7079.3257]], dtype=float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tmp3, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3cf68a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 3)\n",
      "(3064, 3)\n",
      "(5, 3064, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tmp.shape)\n",
    "print(tmp2.shape)\n",
    "print(tmp3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82037e4",
   "metadata": {},
   "source": [
    "So clearly almost all of the time comes from the line calculating u. There is some extra time elsewhere but I just gotta solve that first. \n",
    "The reduce sum line is the bulk of it but the rest takes work too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad8c0a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3064, 3)\n",
      "(5, 3064)\n",
      "tf.Tensor(\n",
      "[[ 11.174502    -2.774537     4.1983414  ...   1.0655003    2.7233696\n",
      "    1.2552919 ]\n",
      " [  1.4182105   -3.418344     3.8420029  ...   5.8528194   -2.1113777\n",
      "   -2.4307096 ]\n",
      " [-13.023096    -7.9385347   -7.853675   ...   5.96973     10.176804\n",
      "    0.7982836 ]\n",
      " [  7.277014   -10.823961     0.4170308  ...   5.4644723    1.9503944\n",
      "  -10.522987  ]\n",
      " [  1.4727321   -4.1790075   -6.7124834  ... -16.180014     7.406987\n",
      "   -0.79876614]], shape=(5, 3064), dtype=float32)\n",
      "tf.Tensor(1.4182105, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(init_state.shape)\n",
    "test = tf.reduce_sum(init_state, 2)\n",
    "test2 = tf.reduce_sum(init_state[1,0,:])\n",
    "print(test.shape)\n",
    "print(test)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0627b07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3064, 3), dtype=float32, numpy=\n",
       "array([[[1.78468496e+04, 7.05183220e+00, 5.66449702e-01],\n",
       "        [1.86681099e+01, 1.51097155e+00, 2.21145060e-03],\n",
       "        [1.02079260e+03, 1.94553822e-01, 3.35227162e-01],\n",
       "        ...,\n",
       "        [3.67204142e+00, 1.29364824e+01, 6.10966012e-02],\n",
       "        [1.11567590e+03, 3.84107661e+00, 3.55429389e-03],\n",
       "        [7.61305261e-03, 6.26841259e+00, 7.35275269e+01]],\n",
       "\n",
       "       [[8.93869340e-01, 1.17332617e+04, 3.93757044e-04],\n",
       "        [9.48317432e+00, 8.55378449e-01, 4.03942866e-03],\n",
       "        [3.05430603e+02, 6.55627623e-02, 2.32804227e+00],\n",
       "        ...,\n",
       "        [3.42543068e+01, 5.78476238e+00, 1.75730228e+00],\n",
       "        [7.19081610e-02, 1.25822008e+00, 1.33815253e+00],\n",
       "        [8.31700420e+00, 2.21257830e+00, 4.78069112e-03]],\n",
       "\n",
       "       [[6.14739547e-04, 1.01133566e-02, 3.55267137e-01],\n",
       "        [3.75151787e+01, 2.70392303e-03, 3.51671316e-03],\n",
       "        [1.79270709e+00, 2.87253875e-02, 7.54079409e-03],\n",
       "        ...,\n",
       "        [2.69139500e+01, 2.90263519e+01, 5.01015306e-01],\n",
       "        [7.76608828e+04, 5.98072052e-01, 5.65944254e-01],\n",
       "        [1.77725494e+00, 3.51733818e+01, 3.55407223e-02]],\n",
       "\n",
       "       [[9.27981720e+01, 1.50691223e+00, 1.03452120e+01],\n",
       "        [3.25962687e+00, 1.25147897e-04, 4.88226786e-02],\n",
       "        [2.57611424e-01, 1.60936582e+00, 3.66011143e+00],\n",
       "        ...,\n",
       "        [6.40235645e+03, 2.78366947e+00, 1.32505065e-02],\n",
       "        [1.97560355e-01, 2.45899722e-01, 1.44739700e+02],\n",
       "        [5.47632694e-01, 1.63267925e-02, 3.00977612e-03]],\n",
       "\n",
       "       [[1.04150772e+00, 1.09850702e+01, 3.81183475e-01],\n",
       "        [7.91269970e+00, 5.90075701e-02, 3.27980258e-02],\n",
       "        [8.03687363e+01, 2.50634272e-02, 6.03500870e-04],\n",
       "        ...,\n",
       "        [1.21692091e-01, 1.70594694e-06, 4.52773720e-01],\n",
       "        [6.89125427e+02, 8.88398730e+03, 2.69096141e-04],\n",
       "        [8.15717163e+01, 6.81993668e-04, 8.08686733e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_phi = tf.math.exp(init_state)\n",
    "exp_phi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
